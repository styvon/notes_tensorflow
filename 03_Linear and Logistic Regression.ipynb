{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 3: Basic Models in TensorFlow\n",
    "## 1. Overview\n",
    "1. Review\n",
    "1. Linear regression in TensorFlow\n",
    "1. Optimizers\n",
    "1. Logistic regression on MNIST\n",
    "1. Loss functions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[github](https://github.com/chiphuyen/tf-stanford-tutorials)\n",
    "- examples: \n",
    "    - 03_linear_regression_starter.py\n",
    "    - 03_logistic_regression_mnist_starter.py\n",
    "- data: fire_theft.xls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Linear regression\n",
    "### example: Chicago fire theft\n",
    "- X: number of incidents of fire\n",
    "- Y: number of incidents of theft\n",
    "- goal: Predict Y from X\n",
    "- model: $Y_{predicted} = w * X + b$  \n",
    "$(Y - Y_{predicted})^2$\n",
    "\n",
    "### Phase 1: Assemble graph\n",
    "- Step 1: Read in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yvonne/Documents/postCampus/MOOC/2017_Stanford_Tensorflow/project/venv/lib/python3.6/site-packages/matplotlib/font_manager.py:280: UserWarning: Matplotlib is building the font cache using fc-list. This may take a moment.\n",
      "  'Matplotlib is building the font cache using fc-list. '\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/fire_theft.xls'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-6f351934c893>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mDATA_FILE\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'data/fire_theft.xls'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mbook\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxlrd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen_workbook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDATA_FILE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding_override\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0msheet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msheet_by_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msheet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrow_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msheet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/postCampus/MOOC/2017_Stanford_Tensorflow/project/venv/lib/python3.6/site-packages/xlrd/__init__.py\u001b[0m in \u001b[0;36mopen_workbook\u001b[0;34m(filename, logfile, verbosity, use_mmap, file_contents, encoding_override, formatting_info, on_demand, ragged_rows)\u001b[0m\n\u001b[1;32m    393\u001b[0m         \u001b[0mpeek\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfile_contents\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mpeeksz\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    394\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 395\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    396\u001b[0m             \u001b[0mpeek\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpeeksz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    397\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mpeek\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34mb\"PK\\x03\\x04\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# a ZIP file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/fire_theft.xls'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import xlrd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DATA_FILE = 'data/fire_theft.xls'\n",
    "\n",
    "book = xlrd.open_workbook(DATA_FILE, encoding_override='utf-8')\n",
    "sheet = book.sheet_by_index(0)\n",
    "data = np.asarray([sheet.row_values(i) for i in range(1, sheet.nrows)])\n",
    "n_samples = sheet.nrows - 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Step 2: Create placeholders for inputs and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = tf.placeholder(tf.float32, name='X')\n",
    "Y = tf.placeholder(tf.float32, name='Y')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Step 3: Create weight and bias  \n",
    "`tf.Variable(initial_value=None, trainable=True, collections=None, name=None, dtype=None, ...)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w = tf.Variable(0.0, name='weights')\n",
    "b = tf.Variable(0.0, name='bias')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Step 4: Build model to predict Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Y_predicted = X * w + b "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Step 5: Specify loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loss = tf.square(Y - Y_predicted, name='loss')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Step 6: Create optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.001).minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Phase 2: Train the model\n",
    "- Initialize variables\n",
    "- Run optimizer op\n",
    "（with data fed into placeholders for inputs and labels）  \n",
    "\n",
    "### See model in TensorBoard\n",
    "- Step 1: `writer = tf.summary.FileWriter('./my_graph/03/linear_reg', sess.graph)`\n",
    "- Step 2: `$ tensorboard --logdir='./my_graph'`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 2069.6319333978354\n",
      "Epoch 1: 2117.0123581953535\n",
      "Epoch 2: 2092.302723001866\n",
      "Epoch 3: 2068.5080461938464\n",
      "Epoch 4: 2045.591184088162\n",
      "Epoch 5: 2023.5146448101316\n",
      "Epoch 6: 2002.2447619835536\n",
      "Epoch 7: 1981.748338803649\n",
      "Epoch 8: 1961.9944411260742\n",
      "Epoch 9: 1942.9520116143283\n",
      "Epoch 10: 1924.5930823644712\n",
      "Epoch 11: 1906.8898800636332\n",
      "Epoch 12: 1889.8164505837929\n",
      "Epoch 13: 1873.347133841543\n",
      "Epoch 14: 1857.4588400604468\n",
      "Epoch 15: 1842.1278742424079\n",
      "Epoch 16: 1827.332495119955\n",
      "Epoch 17: 1813.0520579712022\n",
      "Epoch 18: 1799.2660847636982\n",
      "Epoch 19: 1785.9562132299961\n",
      "Epoch 20: 1773.1024853109072\n",
      "Epoch 21: 1760.689129482884\n",
      "Epoch 22: 1748.6984157081515\n",
      "Epoch 23: 1737.1138680398553\n",
      "Epoch 24: 1725.920873066732\n",
      "Epoch 25: 1715.1046249579008\n",
      "Epoch 26: 1704.6500954309377\n",
      "Epoch 27: 1694.5447134910141\n",
      "Epoch 28: 1684.7746311347667\n",
      "Epoch 29: 1675.328450968245\n",
      "Epoch 30: 1666.1935385839038\n",
      "Epoch 31: 1657.3584002084322\n",
      "Epoch 32: 1648.8122658529207\n",
      "Epoch 33: 1640.5440742547091\n",
      "Epoch 34: 1632.5446836102221\n",
      "Epoch 35: 1624.8043315147183\n",
      "Epoch 36: 1617.3126799958602\n",
      "Epoch 37: 1610.0622532456405\n",
      "Epoch 38: 1603.0433557207386\n",
      "Epoch 39: 1596.2479176106197\n",
      "Epoch 40: 1589.668056331575\n",
      "Epoch 41: 1583.2965242617897\n",
      "Epoch 42: 1577.126371285745\n",
      "Epoch 43: 1571.1501190634\n",
      "Epoch 44: 1565.360979151513\n",
      "Epoch 45: 1559.7523780798629\n",
      "Epoch 46: 1554.3184364555138\n",
      "Epoch 47: 1549.0529469620615\n",
      "Epoch 48: 1543.950059985476\n",
      "Epoch 49: 1539.0050282141283\n",
      "Epoch 50: 1534.211797797609\n",
      "Epoch 51: 1529.56534988646\n",
      "Epoch 52: 1525.0607591186251\n",
      "Epoch 53: 1520.6934648507852\n",
      "Epoch 54: 1516.4585935090713\n",
      "Epoch 55: 1512.3524023861364\n",
      "Epoch 56: 1508.3695780125756\n",
      "Epoch 57: 1504.5066588066873\n",
      "Epoch 58: 1500.7606269073274\n",
      "Epoch 59: 1497.126336559476\n",
      "Epoch 60: 1493.600210891061\n",
      "Epoch 61: 1490.1794991287668\n",
      "Epoch 62: 1486.8605145300749\n",
      "Epoch 63: 1483.639419928193\n",
      "Epoch 64: 1480.5144186365596\n",
      "Epoch 65: 1477.4811065652452\n",
      "Epoch 66: 1474.5376660533782\n",
      "Epoch 67: 1471.6799176652871\n",
      "Epoch 68: 1468.9063155567717\n",
      "Epoch 69: 1466.2136880280007\n",
      "Epoch 70: 1463.5996563179153\n",
      "Epoch 71: 1461.0614086978492\n",
      "Epoch 72: 1458.597208841216\n",
      "Epoch 73: 1456.2043069711044\n",
      "Epoch 74: 1453.8807724802089\n",
      "Epoch 75: 1451.6242183893032\n",
      "Epoch 76: 1449.432753210976\n",
      "Epoch 77: 1447.3042320180018\n",
      "Epoch 78: 1445.237068621615\n",
      "Epoch 79: 1443.228872676177\n",
      "Epoch 80: 1441.2782130186733\n",
      "Epoch 81: 1439.3831422174615\n",
      "Epoch 82: 1437.542224922173\n",
      "Epoch 83: 1435.7540219968096\n",
      "Epoch 84: 1434.0160684508405\n",
      "Epoch 85: 1432.3276573866606\n",
      "Epoch 86: 1430.687153330871\n",
      "Epoch 87: 1429.093016880254\n",
      "Epoch 88: 1427.543719962062\n",
      "Epoch 89: 1426.038033108981\n",
      "Epoch 90: 1424.5748210840281\n",
      "Epoch 91: 1423.1531702368743\n",
      "Epoch 92: 1421.771026852585\n",
      "Epoch 93: 1420.4274983895677\n",
      "Epoch 94: 1419.121967994741\n",
      "Epoch 95: 1417.85251878131\n",
      "Epoch 96: 1416.618930517208\n",
      "Epoch 97: 1415.4196022436731\n",
      "Epoch 98: 1414.2534379121803\n",
      "Epoch 99: 1413.1202843011845\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "\t# Step 7: initialize the necessary variables, in this case, w and b\n",
    "\tsess.run(tf.global_variables_initializer()) \n",
    "\t\n",
    "\twriter = tf.summary.FileWriter('./my_graph/03/linear_reg', sess.graph)\n",
    "\t\n",
    "\t# Step 8: train the model\n",
    "\tfor i in range(100): # train the model 100 times\n",
    "\t\ttotal_loss = 0\n",
    "\t\tfor x, y in data:\n",
    "\t\t\t# Session runs train_op and fetch values of loss\n",
    "\t\t\t_, l = sess.run([optimizer, loss], feed_dict={X: x, Y:y}) \n",
    "\t\t\ttotal_loss += l\n",
    "\t\tprint('Epoch {0}: {1}'.format(i, total_loss/n_samples))\n",
    "\n",
    "\t# close the writer when you're done using it\n",
    "\twriter.close() \n",
    "\t\n",
    "\t# Step 9: output the values of w and b\n",
    "\tw_value, b_value = sess.run([w, b]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the results with matplotlib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD8CAYAAAB5Pm/hAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x107ba1710>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "X, Y = data.T[0], data.T[1]\n",
    "plt.plot(X, Y, 'bo', label='Real data')\n",
    "plt.plot(X, X * w_value + b_value, 'r', label='Predicted data')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Optimizer\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.001).minimize(loss)\n",
    "    _, l = sess.run([optimizer, loss], feed_dict={X: x, Y:y})\n",
    "Session looks at all **trainable** variables that loss depends on and update them\n",
    "### list of optimizers in TF\n",
    "- tf.train.GradientDescentOptimizer\n",
    "- tf.train.AdagradOptimizer\n",
    "- tf.train.MomentumOptimizer\n",
    "- tf.train.AdamOptimizer\n",
    "- tf.train.ProximalGradientDescentOptimizer\n",
    "- tf.train.ProximalAdagradOptimizer\n",
    "- tf.train.RMSPropOptimizer  \n",
    "And more  \n",
    "### improve model: Huber loss\n",
    "- Robust to outliers\n",
    "- Intuition:  \n",
    "    - if the difference between the predicted value and the real value is small, square it  \n",
    "    - If it’s large, take its absolute value\n",
    "- Implementation: can't write $if\\ Y - Y_{predicted} < delta$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def huber_loss(labels, predictions, delta=1.0):\n",
    "    residual = tf.abs(predictions - labels)\n",
    "    condition = tf.less(residual, delta)\n",
    "    small_res = 0.5 * tf.square(residual)\n",
    "    large_res = delta * residual - 0.5 * tf.square(delta)\n",
    "    return tf.select(condition, small_res, large_res)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Logistic Regression\n",
    "### MNIST Database\n",
    "![MNIST](figures/03_01.png)  \n",
    "\n",
    "- Each image is a 28x28 array, flattened out to be a 1-d tensor of size 784\n",
    "- X: image of a handwritten digit\n",
    "- Y: the digit value\n",
    "- goal: Recognize the digit in the image\n",
    "- model: \n",
    "    - logits = X * w + b\n",
    "    - Y_predicted = softmax(logits)\n",
    "    - loss = cross_entropy(Y, Y_predicted)\n",
    "    \n",
    "### Batch ‘em up  \n",
    "    X = tf.placeholder(tf.float32, [batch_size, 784], name=\"image\")\n",
    "    Y = tf.placeholder(tf.float32, [batch_size, 10], name=\"label\")\n",
    "### Process data\n",
    "    from tensorflow.examples.tutorials.mnist import input_data\n",
    "    \n",
    "    # using TF Learn's built in function to load MNIST data to the folder data/mnist\n",
    "    MNIST = input_data.read_data_sets(\"/data/mnist\", one_hot=True)\n",
    "    \n",
    "    # MNIST.train: 55,000 examples\n",
    "    # MNIST.validation: 5,000 examples\n",
    "    # MNIST.test: 10,000 examples\n",
    "    \n",
    "### Phase 1: Assemble our graph\n",
    "- Step 2: Create placeholders for inputs and labels\n",
    "        X = tf.placeholder(tf.float32, [batch_size, 784], name=\"image\")\n",
    "        Y = tf.placeholder(tf.float32, [batch_size, 10], name=\"label\")\n",
    "- Step 3: Create weight and bias\n",
    "        w = tf.Variable(tf.random_normal(shape=[784, 10], stddev=0.01), name='weights')\n",
    "        b = tf.Variable(tf.zeros([1, 10]), name=\"bias\")\n",
    "- Step 4: Build model to predict Y  \n",
    "        # the model that returns the logits.\n",
    "            # this logits will be later passed through softmax layer\n",
    "        logits = tf.matmul(X, w) + b \n",
    "- Step 5: Specify loss function \n",
    "        # use cross entropy of softmax of logits as the loss function\n",
    "        entropy = tf.nn.softmax_cross_entropy_with_logits(logits, Y, name='loss')\n",
    "        loss = tf.reduce_mean(entropy) # computes the mean over all the examples in the batch\n",
    "- Step 6: Create optimizer\n",
    "        optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)\n",
    "        \n",
    "    - add new operations to the computation graph: ones to compute gradients, compute parameter update steps, and apply update steps to the parameters\n",
    "    - when run, will apply the gradient descent updates to the parameters. Training the model can therefore be accomplished by repeatedly running optimizer\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Phase 2: Train our model\n",
    "\n",
    "(https://www.tensorflow.org/get_started/mnist/pros)  \n",
    "\n",
    "        with tf.Session() as sess:\n",
    "            # to visualize using TensorBoard\n",
    "            writer = tf.summary.FileWriter('./my_graph/03/logistic_reg', sess.graph)\n",
    "\n",
    "            start_time = time.time()\n",
    "            sess.run(tf.global_variables_initializer())\t\n",
    "            n_batches = int(mnist.train.num_examples/batch_size)\n",
    "            for i in range(n_epochs): # train the model n_epochs times\n",
    "                total_loss = 0\n",
    "\n",
    "                for _ in range(n_batches):\n",
    "                    X_batch, Y_batch = mnist.train.next_batch(batch_size)\n",
    "                    _, loss_batch = sess.run([optimizer, loss], feed_dict={X: X_batch, Y:Y_batch}) \n",
    "                    total_loss += loss_batch\n",
    "                print 'Average loss epoch {0}: {1}'.format(i, total_loss/n_batches)\n",
    "\n",
    "            print 'Total time: {0} seconds'.format(time.time() - start_time)\n",
    "\n",
    "            print('Optimization Finished!') # should be around 0.35 after 25 epochs\n",
    "\n",
    "            # test the model\n",
    "            n_batches = int(mnist.test.num_examples/batch_size)\n",
    "            total_correct_preds = 0\n",
    "            for i in range(n_batches):\n",
    "                X_batch, Y_batch = mnist.test.next_batch(batch_size)\n",
    "                _, loss_batch, logits_batch = sess.run([optimizer, loss, logits], feed_dict={X: X_batch, Y:Y_batch}) \n",
    "                preds = tf.nn.softmax(logits_batch)\n",
    "                correct_preds = tf.equal(tf.argmax(preds, 1), tf.argmax(Y_batch, 1))\n",
    "                accuracy = tf.reduce_sum(tf.cast(correct_preds, tf.float32)) # need numpy.count_nonzero(boolarr) :(\n",
    "                total_correct_preds += sess.run(accuracy)\t\n",
    "\n",
    "            print 'Accuracy {0}'.format(total_correct_preds/mnist.test.num_examples)\n",
    "\n",
    "            writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
