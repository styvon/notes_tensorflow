{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 11 Introduction to RNNs\n",
    "## 0. Overview\n",
    "- All about RNNs\n",
    "- LSTM, GRU\n",
    "- application of RNNs\n",
    "- RNNs in TensorFlow: implementation tricks & treats\n",
    "- Live demo of Language Modeling\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. RNNs\n",
    "### 1.1 From feed-forward to RNNs\n",
    "- feed-forward network\n",
    "- [Recurrent neural network (RNN)](https://en.wikipedia.org/wiki/Recurrent_neural_network)\n",
    "    - take advantage of sequential information of data (texts, genomes, spoken words, etc.)\n",
    "    - Directed cycles\n",
    "    - All steps share weights to reduce the total number of parameters\n",
    "    - Form the backbone of NLP\n",
    "    - Can also be used for images\n",
    "![rnn_structure_from_nature](figures/11_01.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Simple Recurrent Neural Network (SRNN)\n",
    "![SRNN](figures/11_02.png)\n",
    "- Introduced by Jeffrey Elman in 1990 (*Elman, Jeffrey L. \"Finding structure in time.\" Cognitive science 14.2 (1990): 179-211*) \n",
    "- aka Elman Network \n",
    "![SRNNwiki](figures/11_03.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 RNNs in the context of NLP\n",
    "![RNNdiagram](figures/11_04.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 The problem with RNNs\n",
    "Not very good at capturing long-term dependencies.  \n",
    "e.g. “I grew up in France… I speak fluent ???”  \n",
    "-> Needs information from way back"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. LSTM & GRU\n",
    "### 2.1 Long Short Term Memory (LSTM)\n",
    "- Control how much of new input to take, how much of the previous hidden state to forget\n",
    "- Closer to how humans process information\n",
    "- Not a new idea (*Hochreiter, Sepp, and Jürgen Schmidhuber. \"Long short-term memory.\" Neural computation 9.8 (1997): 1735-1780.*)  \n",
    "![LSTM1](figures/11_05.png)\n",
    "![LSTM2](figures/11_06.png)\n",
    "\n",
    "People find LSTMs work well, but unnecessarily complicated, so they introduced GRUs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 GRUs (Gated Recurrent Units)\n",
    "- two most widely used GRUs\n",
    "![GRUcs224d](figures/11_07.png)\n",
    "- Computationally less expensive\n",
    "- Performance on par with LSTMs (*Chung, Junyoung, et al. \"Empirical evaluation of gated recurrent neural networks on sequence modeling.\" arXiv preprint arXiv:1412.3555 (2014).*)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Applications of RNNs\n",
    "### 3.1 Language Modeling\n",
    "- Allows us to measure how likely a sentence is\n",
    "- Important input for Machine Translation (since high-probability sentences are typically correct)\n",
    "- Can generate new text\n",
    "- e.g Character-level Language Modeling: [Shakespeare Generator & Linux Source Code Generator](http://karpathy.github.io/2015/05/21/rnn-effectiveness/) with [code](https://github.com/karpathy/char-rnn)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 3.2 Machine Translation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Google Neural Machine Translation ([Google Research’s blog](https://research.google.com/pubs/pub45610.html))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Text Summarization\n",
    "- *Nallapati, Ramesh, et al. \"Abstractive text summarization using sequence-to-sequence rnns and beyond.\" arXiv preprint arXiv:1602.06023 (2016).*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Image Captioning\n",
    "- *Karpathy, Andrej, and Li Fei-Fei. \"Deep visual-semantic alignments for generating image descriptions.\" Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2015.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. RNNs in TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 4.1 cell support\n",
    "- `BasicRNNCell`: The most basic RNN cell\n",
    "- `RNNCell`: Abstract object representing an RNN cell\n",
    "- `BasicLSTMCell`: Basic LSTM recurrent network cell\n",
    "- `LSTMCell`: LSTM recurrent network cell\n",
    "- `GRUCell`: Gated Recurrent Unit cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Construct Cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cell = tf.nn.rnn_cell.GRUCell(hidden_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Stack multiple cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cell = tf.nn.rnn_cell.GRUCell(hidden_size)\n",
    "rnn_cells = tf.nn.rnn_cell.MultiRNNCell([cell] * num_layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Construct Recurrent Neural Network\n",
    "- `tf.nn.dynamic_rnn`: uses a `tf.While` loop to dynamically construct the graph when it is executed\n",
    "    - Graph creation is faster\n",
    "    - can feed batches of variable size\n",
    "- `tf.nn.bidirectional_dynamic_rnn`:  dynamic_rnn with bidirectional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cell = tf.nn.rnn_cell.GRUCell(hidden_size)\n",
    "rnn_cells = tf.nn.rnn_cell.MultiRNNCell([cell] * num_layers)\n",
    "output, out_state = tf.nn.dynamic_rnn(cell, seq, length, initial_state) \n",
    "# problem: most sequences are not of the same LENGTH!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5 Dealing with variable sequence length\n",
    "#### Padded/truncated sequence length\n",
    "- Pad all sequences with zero vectors and all labels with zero label (to make them of the same length)\n",
    "- Most current models can’t deal with sequences of length larger than 120 tokens, so there is usually a fixed max_length and we truncate the sequences to that max_length\n",
    "- **problem:** padded labels change the total loss, which affects the gradients "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Approach 1\n",
    "- Maintain a mask (True for real, False for padded tokens)\n",
    "- Run your model on both the real/padded tokens (model will predict labels for the padded tokens as well)\n",
    "- Only take into account the loss caused by the real elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "full_loss = tf.nn.softmax_cross_entropy_with_logits(preds, labels)\n",
    "loss = tf.reduce_mean(tf.boolean_mask(full_loss, mask))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Approach 2\n",
    "- Let your model know the real sequence length so it only predict the labels for the real tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cell = tf.nn.rnn_cell.GRUCell(hidden_size)\n",
    "rnn_cells = tf.nn.rnn_cell.MultiRNNCell([cell] * num_layers)\n",
    "tf.reduce_sum(tf.reduce_max(tf.sign(seq), 2), 1)\n",
    "output, out_state = tf.nn.dynamic_rnn(cell, seq, length, initial_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.6 deal with common problems when training RNNS\n",
    "#### Vanishing Gradients\n",
    "Use different activation units\n",
    "- `tf.nn.relu`\n",
    "- `tf.nn.relu6`\n",
    "- `tf.nn.crelu`\n",
    "- `tf.nn.elu`  \n",
    "  \n",
    "In addition to:\n",
    "- `tf.nn.softplus`\n",
    "- `tf.nn.softsign`\n",
    "- `tf.nn.bias_add`\n",
    "- `tf.sigmoid`\n",
    "- `tf.tanh`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exploding Gradients\n",
    "Clip gradients with `tf.clip_by_global_norm`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# take gradients of cost w.r.t. all trainable variables\n",
    "gradients = tf.gradients(cost, tf.trainable_variables())\n",
    "\n",
    "# clip the gradients by a pre-defined max norm\n",
    "clipped_gradients, _ = tf.clip_by_global_norm(gradients, max_grad_norm)\n",
    "\n",
    "# add the clipped gradients to the optimizer\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "train_op = optimizer.apply_gradients(zip(clipped_gradients, trainables))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Anneal the learning rate\n",
    "Optimizers accept both scalars and tensors as learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learning_rate = tf.train.exponential_decay(init_lr,\n",
    "                                           global_step,\n",
    "                                           decay_steps,\n",
    "                                           decay_rate,\n",
    "                                           staircase=True)\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Overfitting\n",
    "- dropout through `tf.nn.dropout`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hidden_layer = tf.nn.dropout(hidden_layer, keep_prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-  `DropoutWrapper` for cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cell = tf.nn.rnn_cell.GRUCell(hidden_size)\n",
    "cell = tf.nn.rnn_cell.DropoutWrapper(cell,\n",
    "                                     output_keep_prob=keep_prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Language Modeling\n",
    "### 5.1 Neural Language Modeling\n",
    "- Allows us to measure how likely a sentence is\n",
    "- Important input for Machine Translation (since high-probability sentences are typically correct)\n",
    "- Can generate new text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Main approaches\n",
    "- Word-level: n-grams\n",
    "    - The traditional approach up until very recently\n",
    "    - Train a model to predict the next word based on previous n-grams\n",
    "    - problems:\n",
    "        - Huge vocabulary\n",
    "        - Can’t generalize to OOV (out of vocabulary)\n",
    "        - Requires a lot of memory\n",
    "- Character-level\n",
    "    - Introduced in the early 2010s\n",
    "    - Both input and output are characters\n",
    "    - Pros:\n",
    "        - very small vocabulary\n",
    "        - Doesn’t require word embeddings\n",
    "        - faster to train\n",
    "    - Cons:\n",
    "        - Low fluency (many words can be gibberish)\n",
    "- Subword-level: somewhere in between the two above\n",
    "    - hybrid: \n",
    "        - word-level by default\n",
    "        - switch to character-level for unknown tokens\n",
    "    - Input and output are subwords\n",
    "    - Keep $W$ most frequent words\n",
    "    - Keep $S$ most frequent syllables\n",
    "    - Split the rest into characters\n",
    "    - Seem to perform better than both word-level and character-level models (*Mikolov, Tomáš, et al. \"Subword language modeling with neural networks.\" preprint(http://www.fit.vutbr.cz/imikolov/rnnlm/char.pdf) (2012).*)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Demo: Character-level Language Modeling\n",
    "Generate fake Arvix abstracts\n",
    "\n",
    "[data/arvix_abstracts.txt](https://github.com/chiphuyen/tf-stanford-tutorials/blob/master/data/arvix_abstracts.txt)  \n",
    "\n",
    "[examples/11_char_nn_gist.py](https://github.com/chiphuyen/tf-stanford-tutorials/blob/master/examples/11_char_rnn_gist.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\" A clean, no_frills character-level generative language model.\n",
    "Created by Danijar Hafner (danijar.com), edited by Chip Huyen\n",
    "for the class CS 20SI: \"TensorFlow for Deep Learning Research\"\n",
    "Based on Andrej Karpathy's blog: \n",
    "http://karpathy.github.io/2015/05/21/rnn-effectiveness/\n",
    "\"\"\"\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL']='2'\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import time\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import utils\n",
    "\n",
    "DATA_PATH = 'data/arvix_abstracts.txt'\n",
    "HIDDEN_SIZE = 200\n",
    "BATCH_SIZE = 64\n",
    "NUM_STEPS = 50\n",
    "SKIP_STEP = 40\n",
    "TEMPRATURE = 0.7\n",
    "LR = 0.003\n",
    "LEN_GENERATED = 300\n",
    "\n",
    "def vocab_encode(text, vocab):\n",
    "    return [vocab.index(x) + 1 for x in text if x in vocab]\n",
    "\n",
    "def vocab_decode(array, vocab):\n",
    "    return ''.join([vocab[x - 1] for x in array])\n",
    "\n",
    "def read_data(filename, vocab, window=NUM_STEPS, overlap=NUM_STEPS//2):\n",
    "    for text in open(filename):\n",
    "        text = vocab_encode(text, vocab)\n",
    "        for start in range(0, len(text) - window, overlap):\n",
    "            chunk = text[start: start + window]\n",
    "            chunk += [0] * (window - len(chunk))\n",
    "            yield chunk\n",
    "\n",
    "def read_batch(stream, batch_size=BATCH_SIZE):\n",
    "    batch = []\n",
    "    for element in stream:\n",
    "        batch.append(element)\n",
    "        if len(batch) == batch_size:\n",
    "            yield batch\n",
    "            batch = []\n",
    "    yield batch\n",
    "\n",
    "def create_rnn(seq, hidden_size=HIDDEN_SIZE):\n",
    "    cell = tf.contrib.rnn.GRUCell(hidden_size)\n",
    "    in_state = tf.placeholder_with_default(\n",
    "            cell.zero_state(tf.shape(seq)[0], tf.float32), [None, hidden_size])\n",
    "    # this line to calculate the real length of seq\n",
    "    # all seq are padded to be of the same length which is NUM_STEPS\n",
    "    length = tf.reduce_sum(tf.reduce_max(tf.sign(seq), 2), 1)\n",
    "    output, out_state = tf.nn.dynamic_rnn(cell, seq, length, in_state)\n",
    "    return output, in_state, out_state\n",
    "\n",
    "def create_model(seq, temp, vocab, hidden=HIDDEN_SIZE):\n",
    "    seq = tf.one_hot(seq, len(vocab))\n",
    "    output, in_state, out_state = create_rnn(seq, hidden)\n",
    "    # fully_connected is syntactic sugar for tf.matmul(w, output) + b\n",
    "    # it will create w and b for us\n",
    "    logits = tf.contrib.layers.fully_connected(output, len(vocab), None)\n",
    "    loss = tf.reduce_sum(tf.nn.softmax_cross_entropy_with_logits(logits=logits[:, :-1], labels=seq[:, 1:]))\n",
    "    # sample the next character from Maxwell-Boltzmann Distribution with temperature temp\n",
    "    # it works equally well without tf.exp\n",
    "    sample = tf.multinomial(tf.exp(logits[:, -1] / temp), 1)[:, 0] \n",
    "    return loss, sample, in_state, out_state\n",
    "\n",
    "def training(vocab, seq, loss, optimizer, global_step, temp, sample, in_state, out_state):\n",
    "    saver = tf.train.Saver()\n",
    "    start = time.time()\n",
    "    with tf.Session() as sess:\n",
    "        writer = tf.summary.FileWriter('graphs/gist', sess.graph)\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "        ckpt = tf.train.get_checkpoint_state(os.path.dirname('checkpoints/arvix/checkpoint'))\n",
    "        if ckpt and ckpt.model_checkpoint_path:\n",
    "            saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "        \n",
    "        iteration = global_step.eval()\n",
    "        for batch in read_batch(read_data(DATA_PATH, vocab)):\n",
    "            batch_loss, _ = sess.run([loss, optimizer], {seq: batch})\n",
    "            if (iteration + 1) % SKIP_STEP == 0:\n",
    "                print('Iter {}. \\n    Loss {}. Time {}'.format(iteration, batch_loss, time.time() - start))\n",
    "                online_inference(sess, vocab, seq, sample, temp, in_state, out_state)\n",
    "                start = time.time()\n",
    "                saver.save(sess, 'checkpoints/arvix/char-rnn', iteration)\n",
    "            iteration += 1\n",
    "\n",
    "def online_inference(sess, vocab, seq, sample, temp, in_state, out_state, seed='T'):\n",
    "    \"\"\" Generate sequence one character at a time, based on the previous character\n",
    "    \"\"\"\n",
    "    sentence = seed\n",
    "    state = None\n",
    "    for _ in range(LEN_GENERATED):\n",
    "        batch = [vocab_encode(sentence[-1], vocab)]\n",
    "        feed = {seq: batch, temp: TEMPRATURE}\n",
    "        # for the first decoder step, the state is None\n",
    "        if state is not None:\n",
    "            feed.update({in_state: state})\n",
    "        index, state = sess.run([sample, out_state], feed)\n",
    "        sentence += vocab_decode(index, vocab)\n",
    "    print(sentence)\n",
    "\n",
    "def main():\n",
    "    vocab = (\n",
    "            \" $%'()+,-./0123456789:;=?ABCDEFGHIJKLMNOPQRSTUVWXYZ\"\n",
    "            \"\\\\^_abcdefghijklmnopqrstuvwxyz{|}\")\n",
    "    seq = tf.placeholder(tf.int32, [None, None])\n",
    "    temp = tf.placeholder(tf.float32)\n",
    "    loss, sample, in_state, out_state = create_model(seq, temp, vocab)\n",
    "    global_step = tf.Variable(0, dtype=tf.int32, trainable=False, name='global_step')\n",
    "    optimizer = tf.train.AdamOptimizer(LR).minimize(loss, global_step=global_step)\n",
    "    utils.make_dir('checkpoints')\n",
    "    utils.make_dir('checkpoints/arvix')\n",
    "    training(vocab, seq, loss, optimizer, global_step, temp, sample, in_state, out_state)\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
