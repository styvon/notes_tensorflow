{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lecture 7: Convnets in TensorFlow\n",
    "### 0. Overview\n",
    "1. Playing with convolutions\n",
    "2. Convolution support in TF\n",
    "3. More MNIST!!!\n",
    "4. Autoencoder\n",
    "5. Interactive coding\n",
    "\n",
    "### 1. Understanding convolutions\n",
    "- [Chris Olah: understanding convolution](http://colah.github.io/posts/2014-07-Understanding-Convolutions/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 2. Convolution support in TF\n",
    "#### 2.1 `tf.nn.conv2d`\n",
    "\n",
    "    tf.nn.conv2d(input, filter, strides, padding, use_cudnn_on_gpu = None , data_format = None , name = None)\n",
    "\n",
    "- Input: Batch size x Height x Width x Channels\n",
    "- Filter: Height x Width x Input Channels x Output Channels ( e.g. `[5, 5, 3, 64]`)\n",
    "- Strides: 4 element 1-D tensor , strides in each direction ( often `[1, 1, 1, 1]` or `[1, 2, 2, 1]`)\n",
    "- Padding: `'SAME'` or `'VALID'`\n",
    "- Data_format: default to NHWC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because of this property of convolution, we can do convolutions without training anything. We can simply choose a kernel and see how that kernel affects our image.\n",
    "- example: the kernel often used for blurring an image\n",
    "![blur](figures/07_01.png)\n",
    "- other popular kernels: in the `kernels.py` file on the [class GitHub repository](https://github.com/chiphuyen/tf-stanford-tutorials)\n",
    "- how to use them: in `07_basic_filters.py`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 other built-in convolutional operations\n",
    "- more details in [official documentation](https://www.tensorflow.org/api_docs/)\n",
    "- `conv2d`: Arbitrary filters that can mix channels together.\n",
    "- `depthwise_conv2d`: Filters that operate on each channel independently.\n",
    "- `separable_conv2d`: A depthwise spatial filter followed by a pointwise filter.\n",
    "\n",
    "In this case, we hard code our kernels. When training a convnet, we don’t know what the values\n",
    "for our kernels and therefore have to figure them out by learning them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Convnet on MNIST\n",
    "- see `07_convnet_mnist.py`\n",
    "- two convolutional layers, each followed by a relu and a maxpool layers, and one fully connected layer\n",
    "![conv_MNIST](figures/07_02.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Variable scope\n",
    "- similar to namespace: e.g. A variable name ‘weights’ in variable scope ‘conv1’ will become ‘conv1-weights’\n",
    "- common practice: create a variable scope for each layer\n",
    "- use `tf.get_variable()` instead of `tf.Variable()`\n",
    "\n",
    "        tf.get_variable(< name >, <shape> , <initializer>)\n",
    "        \n",
    "- If a variable with that name already exists in that variable scope, we use that variable. If a variable with that name doesn’t already exists in that variable scope, TensorFlow creates a new variable.\n",
    "- Nodes in the same variable scope will be grouped together, and therefore you don’t have to use name scope any more\n",
    "- delare variable scope:\n",
    "\n",
    "        with tf.variable_scope('conv1') as scope:\n",
    "\n",
    "            w = tf.get_variable('weights', [5, 5, 1, 32])\n",
    "            b = tf.get_variable('biases', [32], initializer=tf.random_normal_initializer())\n",
    "            conv = tf.nn.conv2d(images, w, strides=[1, 1, 1, 1], padding='SAME')\n",
    "            conv1 = tf.nn.relu(conv + b, name=scope.name)\n",
    "\n",
    "        with tf.variable_scope('conv2') as scope:\n",
    "\n",
    "            w = tf.get_variable('weights', [5, 5, 32, 64])\n",
    "            b = tf.get_variable('biases', [64], initializer=tf.random_normal_initializer())\n",
    "            conv = tf.nn.conv2d(conv1, w, strides=[1, 1, 1, 1], padding='SAME')\n",
    "            conv2 = tf.nn.relu(conv + b, name=scope.name)\n",
    "    \n",
    "- more resources in [official doc](https://www.tensorflow.org/programmers_guide/variable_scope)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 5. Autoencoder\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Codes\n",
    "- also see [official tutorial](https://www.tensorflow.org/get_started/mnist/beginners)\n",
    "- article on [understanding CNN](https://adeshpande3.github.io/adeshpande3.github.io/A-Beginner's-Guide-To-Understanding-Convolutional-Neural-Networks/)\n",
    "- [visual information theory](http://colah.github.io/posts/2015-09-Visual-Information/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\" Using convolutional net on MNIST dataset of handwritten digit\n",
    "(http://yann.lecun.com/exdb/mnist/)\n",
    "\"\"\"\n",
    "from __future__ import print_function\n",
    "\n",
    "import os\n",
    "import time \n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "N_CLASSES = 10\n",
    "dir = os.getcwd() + \"/data/mnist\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /Users/yvonne/Documents/postCampus/MOOC/2017_Stanford_Tensorflow/project/data/mnist/train-images-idx3-ubyte.gz\n",
      "Extracting /Users/yvonne/Documents/postCampus/MOOC/2017_Stanford_Tensorflow/project/data/mnist/train-labels-idx1-ubyte.gz\n",
      "Extracting /Users/yvonne/Documents/postCampus/MOOC/2017_Stanford_Tensorflow/project/data/mnist/t10k-images-idx3-ubyte.gz\n",
      "Extracting /Users/yvonne/Documents/postCampus/MOOC/2017_Stanford_Tensorflow/project/data/mnist/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Read in data\n",
    "# using TF Learn's built in function to load MNIST data to the folder data/mnist\n",
    "mnist = input_data.read_data_sets(dir, one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Step 2: Define paramaters for the model\n",
    "LEARNING_RATE = 0.001\n",
    "BATCH_SIZE = 128\n",
    "SKIP_STEP = 10\n",
    "DROPOUT = 0.75\n",
    "N_EPOCHS = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Step 3: create placeholders for features and labels\n",
    "# each image in the MNIST data is of shape 28*28 = 784\n",
    "# therefore, each image is represented with a 1x784 tensor\n",
    "# We'll be doing dropout for hidden layer so we'll need a placeholder\n",
    "# for the dropout probability too\n",
    "# Use None for shape so we can change the batch_size once we've built the graph\n",
    "with tf.name_scope('data'):\n",
    "    X = tf.placeholder(tf.float32, [None, 784], name=\"X_placeholder\") # image, not explore 2d structure here (later maybe)\n",
    "    Y = tf.placeholder(tf.float32, [None, 10], name=\"Y_placeholder\") # label(one hot)\n",
    "\n",
    "dropout = tf.placeholder(tf.float32, name='dropout')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4 + 5: create weights + do inference\n",
    "# the model is conv -> relu -> pool -> conv -> relu -> pool -> fully connected -> softmax\n",
    "\n",
    "global_step = tf.Variable(0, dtype=tf.int32, trainable=False, name='global_step')\n",
    "\n",
    "with tf.variable_scope('conv1') as scope:\n",
    "    # first, reshape the image to [BATCH_SIZE, 28, 28, 1] to make it work with tf.nn.conv2d\n",
    "    # use the dynamic dimension -1\n",
    "    \n",
    "    images = tf.reshape(X, shape=[-1, 28, 28, 1])\n",
    "\n",
    "    # create kernel variable of dimension [5, 5, 1, 32]\n",
    "    # use tf.truncated_normal_initializer()\n",
    "    \n",
    "    kernel = tf.get_variable('kernel', [5, 5, 1, 32], initializer=tf.truncated_normal_initializer())\n",
    "\n",
    "    # create biases variable of dimension [32]\n",
    "    # use tf.random_normal_initializer()\n",
    "    \n",
    "    biases = tf.get_variable('biases', [32], initializer=tf.random_normal_initializer()) \n",
    "\n",
    "    # apply tf.nn.conv2d. strides [1, 1, 1, 1], padding is 'SAME'\n",
    "    \n",
    "    conv = tf.nn.conv2d(images, kernel, strides=[1, 1, 1, 1], padding='SAME')\n",
    "\n",
    "    # apply relu on the sum of convolution output and biases\n",
    "    \n",
    "    conv1 = tf.nn.relu(conv + biases, name=scope.name)\n",
    "\n",
    "    # output is of dimension BATCH_SIZE x 28 x 28 x 32\n",
    "\n",
    "with tf.variable_scope('pool1') as scope:\n",
    "    # apply max pool with ksize [1, 2, 2, 1], and strides [1, 2, 2, 1], padding 'SAME'\n",
    "    \n",
    "    pool1 = tf.nn.max_pool(conv1, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "\n",
    "    # output is of dimension BATCH_SIZE x 14 x 14 x 32\n",
    "\n",
    "with tf.variable_scope('conv2') as scope:\n",
    "    # similar to conv1, except kernel now is of the size 5 x 5 x 32 x 64\n",
    "    kernel = tf.get_variable('kernels', [5, 5, 32, 64], \n",
    "                        initializer=tf.truncated_normal_initializer())\n",
    "    biases = tf.get_variable('biases', [64],\n",
    "                        initializer=tf.random_normal_initializer())\n",
    "    conv = tf.nn.conv2d(pool1, kernel, strides=[1, 1, 1, 1], padding='SAME')\n",
    "    conv2 = tf.nn.relu(conv + biases, name=scope.name)\n",
    "\n",
    "    # output is of dimension BATCH_SIZE x 14 x 14 x 64\n",
    "\n",
    "with tf.variable_scope('pool2') as scope:\n",
    "    # similar to pool1\n",
    "    pool2 = tf.nn.max_pool(conv2, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1],\n",
    "                            padding='SAME')\n",
    "\n",
    "    # output is of dimension BATCH_SIZE x 7 x 7 x 64\n",
    "\n",
    "with tf.variable_scope('fc') as scope:\n",
    "    # use weight of dimension 7 * 7 * 64 x 1024\n",
    "    input_features = 7 * 7 * 64\n",
    "    \n",
    "    # create weights and biases\n",
    "\n",
    "    w = tf.get_variable('weights', [input_features, 1024], initializer=tf.truncated_normal_initializer())\n",
    "    b = tf.get_variable('biases', [1024], initializer=tf.random_normal_initializer())\n",
    "\n",
    "    # reshape pool2 to 2 dimensional\n",
    "    pool2 = tf.reshape(pool2, [-1, input_features])\n",
    "\n",
    "    # apply relu on matmul of pool2 and w + b\n",
    "    \n",
    "    fc = tf.nn.relu(tf.matmul(pool2, w) + b, name='relu')\n",
    "\n",
    "    # apply dropout\n",
    "    fc = tf.nn.dropout(fc, dropout, name='relu_dropout')\n",
    "\n",
    "with tf.variable_scope('softmax_linear') as scope:\n",
    "    # this you should know. get logits without softmax\n",
    "    # you need to create weights and biases\n",
    "\n",
    "    w = tf.get_variable('weights', [1024, N_CLASSES], initializer=tf.truncated_normal_initializer())\n",
    "    b = tf.get_variable('biases', [N_CLASSES], initializer=tf.random_normal_initializer())\n",
    "    logits = tf.matmul(fc, w) + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: define loss function\n",
    "# use softmax cross entropy with logits as the loss function\n",
    "# compute mean cross entropy, softmax is applied internally\n",
    "with tf.name_scope('loss'):\n",
    "    # you should know how to do this too\n",
    "    \n",
    "    entropy = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=Y)\n",
    "    loss = tf.reduce_mean(entropy, name='loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Step 7: define training op\n",
    "# using gradient descent with learning rate of LEARNING_RATE to minimize cost\n",
    "# don't forgot to pass in global_step\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(LEARNING_RATE).minimize(loss, global_step=global_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss at step 10: 31006.7\n",
      "Average loss at step 20: 17719.7\n",
      "Average loss at step 30: 10661.2\n",
      "Average loss at step 40: 6685.0\n",
      "Average loss at step 50: 5475.7\n",
      "Average loss at step 60: 5111.7\n",
      "Average loss at step 70: 4024.8\n",
      "Average loss at step 80: 3439.6\n",
      "Average loss at step 90: 2959.4\n",
      "Average loss at step 100: 2852.5\n",
      "Average loss at step 110: 2903.5\n",
      "Average loss at step 120: 2380.3\n",
      "Average loss at step 130: 2140.8\n",
      "Average loss at step 140: 2300.8\n",
      "Average loss at step 150: 2371.5\n",
      "Average loss at step 160: 1865.8\n",
      "Average loss at step 170: 1535.0\n",
      "Average loss at step 180: 1710.7\n",
      "Average loss at step 190: 1682.0\n",
      "Average loss at step 200: 1493.5\n",
      "Average loss at step 210: 1472.5\n",
      "Average loss at step 220: 1207.6\n",
      "Average loss at step 230: 1059.6\n",
      "Average loss at step 240: 1049.6\n",
      "Average loss at step 250: 1282.4\n",
      "Average loss at step 260: 1142.0\n",
      "Average loss at step 270: 1144.7\n",
      "Average loss at step 280: 1298.7\n",
      "Average loss at step 290: 1017.7\n",
      "Average loss at step 300: 1153.4\n",
      "Average loss at step 310: 1137.6\n",
      "Average loss at step 320: 946.5\n",
      "Average loss at step 330: 930.7\n",
      "Average loss at step 340: 806.2\n",
      "Average loss at step 350: 781.7\n",
      "Average loss at step 360: 803.9\n",
      "Average loss at step 370: 821.9\n",
      "Average loss at step 380: 888.7\n",
      "Average loss at step 390: 850.7\n",
      "Average loss at step 400: 658.0\n",
      "Average loss at step 410: 644.7\n",
      "Average loss at step 420: 567.3\n",
      "Average loss at step 430: 750.4\n",
      "Average loss at step 440: 601.8\n",
      "Average loss at step 450: 654.3\n",
      "Average loss at step 460: 691.1\n",
      "Average loss at step 470: 576.3\n",
      "Average loss at step 480: 591.9\n",
      "Average loss at step 490: 612.6\n",
      "Average loss at step 500: 632.8\n",
      "Average loss at step 510: 558.1\n",
      "Average loss at step 520: 692.1\n",
      "Average loss at step 530: 537.8\n",
      "Average loss at step 540: 539.0\n",
      "Average loss at step 550: 517.3\n",
      "Average loss at step 560: 420.2\n",
      "Average loss at step 570: 406.6\n",
      "Average loss at step 580: 418.2\n",
      "Average loss at step 590: 462.4\n",
      "Average loss at step 600: 338.7\n",
      "Average loss at step 610: 491.5\n",
      "Average loss at step 620: 427.3\n",
      "Average loss at step 630: 374.7\n",
      "Average loss at step 640: 389.2\n",
      "Average loss at step 650: 403.4\n",
      "Average loss at step 660: 478.7\n",
      "Average loss at step 670: 427.8\n",
      "Average loss at step 680: 308.0\n",
      "Average loss at step 690: 443.3\n",
      "Average loss at step 700: 520.4\n",
      "Average loss at step 710: 422.6\n",
      "Average loss at step 720: 440.3\n",
      "Average loss at step 730: 366.0\n",
      "Average loss at step 740: 391.5\n",
      "Average loss at step 750: 357.8\n",
      "Average loss at step 760: 345.5\n",
      "Average loss at step 770: 311.6\n",
      "Average loss at step 780: 411.6\n",
      "Average loss at step 790: 406.4\n",
      "Average loss at step 800: 305.2\n",
      "Average loss at step 810: 418.9\n",
      "Average loss at step 820: 347.6\n",
      "Average loss at step 830: 240.8\n",
      "Average loss at step 840: 262.5\n",
      "Average loss at step 850: 374.0\n",
      "Average loss at step 860: 337.4\n",
      "Average loss at step 870: 230.2\n",
      "Average loss at step 880: 342.0\n",
      "Average loss at step 890: 198.7\n",
      "Average loss at step 900: 274.6\n",
      "Average loss at step 910: 273.0\n",
      "Average loss at step 920: 154.7\n",
      "Average loss at step 930: 303.8\n",
      "Average loss at step 940: 300.4\n",
      "Average loss at step 950: 230.8\n",
      "Average loss at step 960: 311.6\n",
      "Average loss at step 970: 196.9\n",
      "Average loss at step 980: 193.9\n",
      "Average loss at step 990: 279.8\n",
      "Average loss at step 1000: 221.9\n",
      "Average loss at step 1010: 329.6\n",
      "Average loss at step 1020: 223.5\n",
      "Average loss at step 1030: 236.2\n",
      "Average loss at step 1040: 227.6\n",
      "Average loss at step 1050: 284.7\n",
      "Average loss at step 1060: 215.0\n",
      "Average loss at step 1070: 185.6\n",
      "Average loss at step 1080: 221.8\n",
      "Average loss at step 1090: 235.6\n",
      "Average loss at step 1100: 225.2\n",
      "Average loss at step 1110: 248.2\n",
      "Average loss at step 1120: 221.7\n",
      "Average loss at step 1130: 289.6\n",
      "Average loss at step 1140: 193.1\n",
      "Average loss at step 1150: 229.1\n",
      "Average loss at step 1160: 206.6\n",
      "Average loss at step 1170: 183.9\n",
      "Average loss at step 1180: 140.9\n",
      "Average loss at step 1190: 155.0\n",
      "Average loss at step 1200: 251.8\n",
      "Average loss at step 1210: 227.6\n",
      "Average loss at step 1220: 292.9\n",
      "Average loss at step 1230: 260.0\n",
      "Average loss at step 1240: 155.4\n",
      "Average loss at step 1250: 177.9\n",
      "Average loss at step 1260: 223.5\n",
      "Average loss at step 1270: 133.3\n",
      "Average loss at step 1280: 220.0\n",
      "Average loss at step 1290: 203.0\n",
      "Average loss at step 1300: 202.6\n",
      "Average loss at step 1310: 168.5\n",
      "Average loss at step 1320: 140.4\n",
      "Average loss at step 1330:  90.3\n",
      "Average loss at step 1340: 153.3\n",
      "Average loss at step 1350: 161.1\n",
      "Average loss at step 1360: 115.4\n",
      "Average loss at step 1370: 182.3\n",
      "Average loss at step 1380: 140.0\n",
      "Average loss at step 1390: 126.4\n",
      "Average loss at step 1400: 182.2\n",
      "Average loss at step 1410: 199.3\n",
      "Average loss at step 1420: 155.3\n",
      "Average loss at step 1430: 149.5\n",
      "Average loss at step 1440: 136.2\n",
      "Average loss at step 1450: 166.8\n",
      "Average loss at step 1460: 262.6\n",
      "Average loss at step 1470: 134.4\n",
      "Average loss at step 1480: 221.6\n",
      "Average loss at step 1490: 185.0\n",
      "Average loss at step 1500: 171.9\n",
      "Average loss at step 1510: 158.3\n",
      "Average loss at step 1520: 107.2\n",
      "Average loss at step 1530: 111.7\n",
      "Average loss at step 1540: 119.3\n",
      "Average loss at step 1550: 109.1\n",
      "Average loss at step 1560: 132.7\n",
      "Average loss at step 1570: 125.7\n",
      "Average loss at step 1580: 168.1\n",
      "Average loss at step 1590: 138.7\n",
      "Average loss at step 1600: 161.7\n",
      "Average loss at step 1610: 152.5\n",
      "Average loss at step 1620: 139.1\n",
      "Average loss at step 1630: 117.2\n",
      "Average loss at step 1640: 124.8\n",
      "Average loss at step 1650: 130.1\n",
      "Average loss at step 1660: 181.3\n",
      "Average loss at step 1670: 171.9\n",
      "Average loss at step 1680: 147.6\n",
      "Average loss at step 1690: 129.2\n",
      "Average loss at step 1700: 164.1\n",
      "Average loss at step 1710: 155.4\n",
      "Average loss at step 1720: 100.7\n",
      "Average loss at step 1730:  73.8\n",
      "Average loss at step 1740: 109.1\n",
      "Average loss at step 1750:  90.2\n",
      "Average loss at step 1760: 141.5\n",
      "Average loss at step 1770:  98.4\n",
      "Average loss at step 1780: 121.3\n",
      "Average loss at step 1790: 157.5\n",
      "Average loss at step 1800: 138.2\n",
      "Average loss at step 1810: 112.0\n",
      "Average loss at step 1820:  98.6\n",
      "Average loss at step 1830: 128.6\n",
      "Average loss at step 1840: 112.7\n",
      "Average loss at step 1850: 114.2\n",
      "Average loss at step 1860:  97.3\n",
      "Average loss at step 1870: 136.9\n",
      "Average loss at step 1880: 130.0\n",
      "Average loss at step 1890: 125.4\n",
      "Average loss at step 1900:  85.5\n",
      "Average loss at step 1910: 139.3\n",
      "Average loss at step 1920:  99.8\n",
      "Average loss at step 1930:  97.6\n",
      "Average loss at step 1940: 122.6\n",
      "Average loss at step 1950:  81.1\n",
      "Average loss at step 1960: 118.5\n",
      "Average loss at step 1970: 125.0\n",
      "Average loss at step 1980: 107.7\n",
      "Average loss at step 1990:  88.5\n",
      "Average loss at step 2000:  68.1\n",
      "Average loss at step 2010: 102.5\n",
      "Average loss at step 2020: 123.1\n",
      "Average loss at step 2030: 112.2\n",
      "Average loss at step 2040:  95.1\n",
      "Average loss at step 2050:  85.3\n",
      "Average loss at step 2060: 110.7\n",
      "Average loss at step 2070: 132.6\n",
      "Average loss at step 2080:  42.1\n",
      "Average loss at step 2090:  88.7\n",
      "Average loss at step 2100: 102.3\n",
      "Average loss at step 2110: 107.0\n",
      "Average loss at step 2120:  83.8\n",
      "Average loss at step 2130:  77.6\n",
      "Average loss at step 2140:  66.4\n",
      "Average loss at step 2150:  74.5\n",
      "Average loss at step 2160:  67.7\n",
      "Average loss at step 2170:  88.5\n",
      "Average loss at step 2180: 100.9\n",
      "Average loss at step 2190:  68.5\n",
      "Average loss at step 2200: 116.9\n",
      "Average loss at step 2210:  57.2\n",
      "Average loss at step 2220:  72.4\n",
      "Average loss at step 2230:  77.7\n",
      "Average loss at step 2240: 114.5\n",
      "Average loss at step 2250:  68.9\n",
      "Average loss at step 2260:  89.3\n",
      "Average loss at step 2270:  81.2\n",
      "Average loss at step 2280:  77.1\n",
      "Average loss at step 2290:  70.5\n",
      "Average loss at step 2300:  71.2\n",
      "Average loss at step 2310:  53.1\n",
      "Average loss at step 2320:  64.5\n",
      "Average loss at step 2330:  73.2\n",
      "Average loss at step 2340:  75.0\n",
      "Average loss at step 2350:  53.6\n",
      "Average loss at step 2360:  77.1\n",
      "Average loss at step 2370:  42.0\n",
      "Average loss at step 2380:  71.8\n",
      "Average loss at step 2390:  63.7\n",
      "Average loss at step 2400:  48.8\n",
      "Average loss at step 2410:  58.9\n",
      "Average loss at step 2420:  48.2\n",
      "Average loss at step 2430: 115.0\n",
      "Average loss at step 2440:  62.0\n",
      "Average loss at step 2450:  66.2\n",
      "Average loss at step 2460:  92.6\n",
      "Average loss at step 2470:  87.6\n",
      "Average loss at step 2480:  68.7\n",
      "Average loss at step 2490: 100.8\n",
      "Average loss at step 2500:  66.1\n",
      "Average loss at step 2510:  84.7\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss at step 2520:  50.8\n",
      "Average loss at step 2530:  85.9\n",
      "Average loss at step 2540:  63.7\n",
      "Average loss at step 2550:  67.9\n",
      "Average loss at step 2560: 108.5\n",
      "Average loss at step 2570:  93.7\n",
      "Average loss at step 2580:  44.8\n",
      "Average loss at step 2590:  54.4\n",
      "Average loss at step 2600:  33.5\n",
      "Average loss at step 2610:  56.3\n",
      "Average loss at step 2620:  44.8\n",
      "Average loss at step 2630:  61.5\n",
      "Average loss at step 2640:  58.4\n",
      "Average loss at step 2650:  35.6\n",
      "Average loss at step 2660:  67.0\n",
      "Average loss at step 2670:  41.0\n",
      "Average loss at step 2680:  53.3\n",
      "Average loss at step 2690:  93.4\n",
      "Average loss at step 2700:  38.6\n",
      "Average loss at step 2710:  55.7\n",
      "Average loss at step 2720:  40.1\n",
      "Average loss at step 2730:  56.2\n",
      "Average loss at step 2740:  91.2\n",
      "Average loss at step 2750:  37.7\n",
      "Average loss at step 2760:  44.1\n",
      "Average loss at step 2770:  46.3\n",
      "Average loss at step 2780:  53.8\n",
      "Average loss at step 2790:  55.3\n",
      "Average loss at step 2800:  65.6\n",
      "Average loss at step 2810:  60.1\n",
      "Average loss at step 2820:  47.5\n",
      "Average loss at step 2830:  67.7\n",
      "Average loss at step 2840:  61.0\n",
      "Average loss at step 2850:  66.3\n",
      "Average loss at step 2860:  26.5\n",
      "Average loss at step 2870:  43.0\n",
      "Average loss at step 2880:  61.3\n",
      "Average loss at step 2890:  54.9\n",
      "Average loss at step 2900:  58.9\n",
      "Average loss at step 2910:  61.8\n",
      "Average loss at step 2920:  64.5\n",
      "Average loss at step 2930:  42.1\n",
      "Average loss at step 2940:  62.1\n",
      "Average loss at step 2950:  40.0\n",
      "Average loss at step 2960:  66.1\n",
      "Average loss at step 2970:  79.2\n",
      "Average loss at step 2980:  53.7\n",
      "Average loss at step 2990:  50.3\n",
      "Average loss at step 3000:  50.1\n",
      "Average loss at step 3010:  42.9\n",
      "Average loss at step 3020:  58.5\n",
      "Average loss at step 3030:  39.8\n",
      "Average loss at step 3040:  53.7\n",
      "Average loss at step 3050:  42.4\n",
      "Average loss at step 3060:  48.3\n",
      "Average loss at step 3070:  33.4\n",
      "Average loss at step 3080:  37.7\n",
      "Average loss at step 3090:  52.4\n",
      "Average loss at step 3100:  53.2\n",
      "Average loss at step 3110:  51.2\n",
      "Average loss at step 3120:  39.1\n",
      "Average loss at step 3130:  47.6\n",
      "Average loss at step 3140:  38.7\n",
      "Average loss at step 3150:  69.2\n",
      "Average loss at step 3160:  39.9\n",
      "Average loss at step 3170:  48.4\n",
      "Average loss at step 3180:  40.4\n",
      "Average loss at step 3190:  54.2\n",
      "Average loss at step 3200:  33.1\n",
      "Average loss at step 3210:  32.5\n",
      "Average loss at step 3220:  26.6\n",
      "Average loss at step 3230:  68.0\n",
      "Average loss at step 3240:  49.0\n",
      "Average loss at step 3250:  41.8\n",
      "Average loss at step 3260:  35.0\n",
      "Average loss at step 3270:  35.1\n",
      "Average loss at step 3280:  29.3\n",
      "Average loss at step 3290:  47.3\n",
      "Average loss at step 3300:  26.7\n",
      "Average loss at step 3310:  14.5\n",
      "Average loss at step 3320:  62.1\n",
      "Average loss at step 3330:  42.7\n",
      "Average loss at step 3340:  47.9\n",
      "Average loss at step 3350:  44.7\n",
      "Average loss at step 3360:  39.9\n",
      "Average loss at step 3370:  48.3\n",
      "Average loss at step 3380:  56.9\n",
      "Average loss at step 3390:  48.3\n",
      "Average loss at step 3400:  39.7\n",
      "Average loss at step 3410:  39.5\n",
      "Average loss at step 3420:  40.2\n",
      "Average loss at step 3430:  56.2\n",
      "Average loss at step 3440:  26.9\n",
      "Average loss at step 3450:  29.5\n",
      "Average loss at step 3460:  32.4\n",
      "Average loss at step 3470:  20.7\n",
      "Average loss at step 3480:  25.2\n",
      "Average loss at step 3490:  15.6\n",
      "Average loss at step 3500:  42.3\n",
      "Average loss at step 3510:  33.8\n",
      "Average loss at step 3520:  40.2\n",
      "Average loss at step 3530:  32.0\n",
      "Average loss at step 3540:  55.6\n",
      "Average loss at step 3550:  22.9\n",
      "Average loss at step 3560:  39.8\n",
      "Average loss at step 3570:  37.0\n",
      "Average loss at step 3580:  28.1\n",
      "Average loss at step 3590:  35.0\n",
      "Average loss at step 3600:  30.9\n",
      "Average loss at step 3610:  36.4\n",
      "Average loss at step 3620:  24.9\n",
      "Average loss at step 3630:  37.9\n",
      "Average loss at step 3640:  26.1\n",
      "Average loss at step 3650:  34.4\n",
      "Average loss at step 3660:  47.9\n",
      "Average loss at step 3670:  40.6\n",
      "Average loss at step 3680:  15.3\n",
      "Average loss at step 3690:  28.5\n",
      "Average loss at step 3700:  37.8\n",
      "Average loss at step 3710:  20.8\n",
      "Average loss at step 3720:  39.2\n",
      "Average loss at step 3730:  41.0\n",
      "Average loss at step 3740:  49.4\n",
      "Average loss at step 3750:  40.6\n",
      "Average loss at step 3760:  37.0\n",
      "Average loss at step 3770:  39.9\n",
      "Average loss at step 3780:  40.9\n",
      "Average loss at step 3790:  30.3\n",
      "Average loss at step 3800:  36.8\n",
      "Average loss at step 3810:  43.4\n",
      "Average loss at step 3820:  29.2\n",
      "Average loss at step 3830:  43.1\n",
      "Average loss at step 3840:  17.9\n",
      "Average loss at step 3850:  30.9\n",
      "Average loss at step 3860:  38.5\n",
      "Average loss at step 3870:  22.8\n",
      "Average loss at step 3880:  21.1\n",
      "Average loss at step 3890:  30.5\n",
      "Average loss at step 3900:  27.2\n",
      "Average loss at step 3910:  27.4\n",
      "Average loss at step 3920:  42.2\n",
      "Average loss at step 3930:  35.7\n",
      "Average loss at step 3940:  24.8\n",
      "Average loss at step 3950:  30.4\n",
      "Average loss at step 3960:  27.1\n",
      "Average loss at step 3970:  21.7\n",
      "Average loss at step 3980:  28.7\n",
      "Average loss at step 3990:  29.6\n",
      "Average loss at step 4000:  30.4\n",
      "Average loss at step 4010:  32.9\n",
      "Average loss at step 4020:  22.4\n",
      "Average loss at step 4030:  28.7\n",
      "Average loss at step 4040:  29.2\n",
      "Average loss at step 4050:  24.4\n",
      "Average loss at step 4060:  38.8\n",
      "Average loss at step 4070:  29.8\n",
      "Average loss at step 4080:  51.2\n",
      "Average loss at step 4090:  30.3\n",
      "Average loss at step 4100:  21.4\n",
      "Average loss at step 4110:  24.5\n",
      "Average loss at step 4120:  42.9\n",
      "Average loss at step 4130:  31.8\n",
      "Average loss at step 4140:  25.0\n",
      "Average loss at step 4150:  48.6\n",
      "Average loss at step 4160:  31.7\n",
      "Average loss at step 4170:  31.4\n",
      "Average loss at step 4180:  41.7\n",
      "Average loss at step 4190:  38.3\n",
      "Average loss at step 4200:  25.8\n",
      "Average loss at step 4210:  27.8\n",
      "Average loss at step 4220:  31.1\n",
      "Average loss at step 4230:  29.6\n",
      "Average loss at step 4240:  45.0\n",
      "Average loss at step 4250:  22.2\n",
      "Average loss at step 4260:  32.4\n",
      "Average loss at step 4270:  29.6\n",
      "Average loss at step 4280:  40.5\n",
      "Average loss at step 4290:  39.8\n",
      "Average loss at step 4300:  28.5\n",
      "Average loss at step 4310:  29.5\n",
      "Average loss at step 4320:  26.3\n",
      "Average loss at step 4330:  12.3\n",
      "Average loss at step 4340:  26.9\n",
      "Average loss at step 4350:  21.3\n",
      "Average loss at step 4360:  23.1\n",
      "Average loss at step 4370:  14.0\n",
      "Average loss at step 4380:  18.7\n",
      "Average loss at step 4390:  29.4\n",
      "Average loss at step 4400:  15.7\n",
      "Average loss at step 4410:  33.7\n",
      "Average loss at step 4420:  19.0\n",
      "Average loss at step 4430:  10.5\n",
      "Average loss at step 4440:  20.6\n",
      "Average loss at step 4450:  32.6\n",
      "Average loss at step 4460:  18.0\n",
      "Average loss at step 4470:  34.2\n",
      "Average loss at step 4480:  32.0\n",
      "Average loss at step 4490:  16.4\n",
      "Average loss at step 4500:  39.4\n",
      "Average loss at step 4510:  18.9\n",
      "Average loss at step 4520:  17.2\n",
      "Average loss at step 4530:  32.5\n",
      "Average loss at step 4540:  19.8\n",
      "Average loss at step 4550:  21.0\n",
      "Average loss at step 4560:  17.9\n",
      "Average loss at step 4570:  37.3\n",
      "Average loss at step 4580:  39.2\n",
      "Average loss at step 4590:  27.9\n",
      "Average loss at step 4600:  27.2\n",
      "Average loss at step 4610:  28.8\n",
      "Average loss at step 4620:  20.5\n",
      "Average loss at step 4630:  16.2\n",
      "Average loss at step 4640:  30.5\n",
      "Average loss at step 4650:  36.1\n",
      "Average loss at step 4660:  28.1\n",
      "Average loss at step 4670:  16.5\n",
      "Average loss at step 4680:  22.9\n",
      "Average loss at step 4690:  24.6\n",
      "Average loss at step 4700:  25.0\n",
      "Average loss at step 4710:  28.9\n",
      "Average loss at step 4720:  26.8\n",
      "Average loss at step 4730:   9.7\n",
      "Average loss at step 4740:  19.1\n",
      "Average loss at step 4750:  18.7\n",
      "Average loss at step 4760:  20.2\n",
      "Average loss at step 4770:  11.6\n",
      "Average loss at step 4780:  13.0\n",
      "Average loss at step 4790:  21.1\n",
      "Average loss at step 4800:  17.0\n",
      "Average loss at step 4810:  25.2\n",
      "Average loss at step 4820:  13.5\n",
      "Average loss at step 4830:  17.9\n",
      "Average loss at step 4840:  21.0\n",
      "Average loss at step 4850:  21.0\n",
      "Average loss at step 4860:  30.9\n",
      "Average loss at step 4870:  14.3\n",
      "Average loss at step 4880:  23.3\n",
      "Average loss at step 4890:  18.1\n",
      "Average loss at step 4900:  16.9\n",
      "Average loss at step 4910:  17.8\n",
      "Average loss at step 4920:  15.2\n",
      "Average loss at step 4930:  19.8\n",
      "Average loss at step 4940:  14.7\n",
      "Average loss at step 4950:  11.0\n",
      "Average loss at step 4960:  20.6\n",
      "Average loss at step 4970:  12.7\n",
      "Average loss at step 4980:  14.9\n",
      "Average loss at step 4990:  15.9\n",
      "Average loss at step 5000:  23.1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss at step 5010:   7.2\n",
      "Average loss at step 5020:  15.3\n",
      "Average loss at step 5030:  27.9\n",
      "Average loss at step 5040:  36.8\n",
      "Average loss at step 5050:  34.3\n",
      "Average loss at step 5060:  32.1\n",
      "Average loss at step 5070:  32.9\n",
      "Average loss at step 5080:  20.4\n",
      "Average loss at step 5090:  16.6\n",
      "Average loss at step 5100:  20.0\n",
      "Average loss at step 5110:  34.0\n",
      "Average loss at step 5120:  21.0\n",
      "Average loss at step 5130:  25.7\n",
      "Average loss at step 5140:  25.5\n",
      "Average loss at step 5150:  26.0\n",
      "Average loss at step 5160:  11.1\n",
      "Average loss at step 5170:  10.9\n",
      "Average loss at step 5180:   7.6\n",
      "Average loss at step 5190:   8.8\n",
      "Average loss at step 5200:  18.3\n",
      "Average loss at step 5210:  16.2\n",
      "Average loss at step 5220:  11.1\n",
      "Average loss at step 5230:  18.9\n",
      "Average loss at step 5240:  19.6\n",
      "Average loss at step 5250:  18.1\n",
      "Average loss at step 5260:  17.6\n",
      "Average loss at step 5270:  17.1\n",
      "Average loss at step 5280:  21.1\n",
      "Average loss at step 5290:  11.3\n",
      "Average loss at step 5300:  15.9\n",
      "Average loss at step 5310:  33.7\n",
      "Average loss at step 5320:   5.6\n",
      "Average loss at step 5330:  22.1\n",
      "Average loss at step 5340:  25.1\n",
      "Average loss at step 5350:  22.2\n",
      "Average loss at step 5360:  17.9\n",
      "Average loss at step 5370:   8.3\n",
      "Average loss at step 5380:  22.3\n",
      "Average loss at step 5390:  27.4\n",
      "Average loss at step 5400:  26.6\n",
      "Average loss at step 5410:  14.9\n",
      "Average loss at step 5420:  15.0\n",
      "Average loss at step 5430:  13.8\n",
      "Average loss at step 5440:  17.7\n",
      "Average loss at step 5450:  23.1\n",
      "Average loss at step 5460:  26.6\n",
      "Average loss at step 5470:  12.7\n",
      "Average loss at step 5480:  20.2\n",
      "Average loss at step 5490:  21.3\n",
      "Average loss at step 5500:  10.0\n",
      "Average loss at step 5510:  18.9\n",
      "Average loss at step 5520:  30.1\n",
      "Average loss at step 5530:  16.7\n",
      "Average loss at step 5540:   8.8\n",
      "Average loss at step 5550:  20.9\n",
      "Average loss at step 5560:  17.9\n",
      "Average loss at step 5570:  20.8\n",
      "Average loss at step 5580:  13.9\n",
      "Average loss at step 5590:  18.0\n",
      "Average loss at step 5600:  13.9\n",
      "Average loss at step 5610:  12.7\n",
      "Average loss at step 5620:  18.0\n",
      "Average loss at step 5630:  18.3\n",
      "Average loss at step 5640:   7.6\n",
      "Average loss at step 5650:  16.0\n",
      "Average loss at step 5660:  15.0\n",
      "Average loss at step 5670:  15.1\n",
      "Average loss at step 5680:  16.0\n",
      "Average loss at step 5690:  10.3\n",
      "Average loss at step 5700:  12.4\n",
      "Average loss at step 5710:  22.5\n",
      "Average loss at step 5720:  12.0\n",
      "Average loss at step 5730:  11.0\n",
      "Average loss at step 5740:  19.0\n",
      "Average loss at step 5750:   6.2\n",
      "Average loss at step 5760:  23.1\n",
      "Average loss at step 5770:  12.6\n",
      "Average loss at step 5780:  14.1\n",
      "Average loss at step 5790:  12.0\n",
      "Average loss at step 5800:  10.8\n",
      "Average loss at step 5810:   9.1\n",
      "Average loss at step 5820:  21.0\n",
      "Average loss at step 5830:  16.4\n",
      "Average loss at step 5840:  18.2\n",
      "Average loss at step 5850:  17.8\n",
      "Average loss at step 5860:  19.7\n",
      "Average loss at step 5870:   5.1\n",
      "Average loss at step 5880:  13.1\n",
      "Average loss at step 5890:  15.2\n",
      "Average loss at step 5900:  21.2\n",
      "Average loss at step 5910:   6.9\n",
      "Average loss at step 5920:  25.5\n",
      "Average loss at step 5930:  16.8\n",
      "Average loss at step 5940:  12.5\n",
      "Average loss at step 5950:   9.8\n",
      "Average loss at step 5960:  18.4\n",
      "Average loss at step 5970:  19.9\n",
      "Average loss at step 5980:  22.9\n",
      "Average loss at step 5990:  12.4\n",
      "Average loss at step 6000:  13.1\n",
      "Average loss at step 6010:  16.3\n",
      "Average loss at step 6020:   9.7\n",
      "Average loss at step 6030:   8.7\n",
      "Average loss at step 6040:  13.5\n",
      "Average loss at step 6050:  13.2\n",
      "Average loss at step 6060:  14.9\n",
      "Average loss at step 6070:   3.9\n",
      "Average loss at step 6080:  18.3\n",
      "Average loss at step 6090:  21.9\n",
      "Average loss at step 6100:   5.6\n",
      "Average loss at step 6110:  15.2\n",
      "Average loss at step 6120:   5.6\n",
      "Average loss at step 6130:  15.6\n",
      "Average loss at step 6140:  21.1\n",
      "Average loss at step 6150:  14.1\n",
      "Average loss at step 6160:  14.2\n",
      "Average loss at step 6170:   9.7\n",
      "Average loss at step 6180:  12.7\n",
      "Average loss at step 6190:  14.6\n",
      "Average loss at step 6200:   7.9\n",
      "Average loss at step 6210:  22.6\n",
      "Average loss at step 6220:  20.9\n",
      "Average loss at step 6230:  11.7\n",
      "Average loss at step 6240:   9.9\n",
      "Average loss at step 6250:   9.3\n",
      "Average loss at step 6260:  11.4\n",
      "Average loss at step 6270:  12.3\n",
      "Average loss at step 6280:  10.3\n",
      "Average loss at step 6290:  11.8\n",
      "Average loss at step 6300:  26.6\n",
      "Average loss at step 6310:  14.1\n",
      "Average loss at step 6320:  10.4\n",
      "Average loss at step 6330:  29.5\n",
      "Average loss at step 6340:  13.7\n",
      "Average loss at step 6350:  12.8\n",
      "Average loss at step 6360:  18.2\n",
      "Average loss at step 6370:  14.5\n",
      "Average loss at step 6380:  14.8\n",
      "Average loss at step 6390:  14.4\n",
      "Average loss at step 6400:   9.6\n",
      "Average loss at step 6410:  19.8\n",
      "Average loss at step 6420:  13.5\n",
      "Average loss at step 6430:   8.9\n",
      "Average loss at step 6440:  13.8\n",
      "Average loss at step 6450:  15.5\n",
      "Average loss at step 6460:  23.9\n",
      "Average loss at step 6470:  12.3\n",
      "Average loss at step 6480:   7.4\n",
      "Average loss at step 6490:  10.6\n",
      "Average loss at step 6500:   8.7\n",
      "Average loss at step 6510:  10.5\n",
      "Average loss at step 6520:   6.3\n",
      "Average loss at step 6530:  10.5\n",
      "Average loss at step 6540:  23.3\n",
      "Average loss at step 6550:  12.4\n",
      "Average loss at step 6560:   9.4\n",
      "Average loss at step 6570:  10.1\n",
      "Average loss at step 6580:  14.7\n",
      "Average loss at step 6590:   6.7\n",
      "Average loss at step 6600:   5.7\n",
      "Average loss at step 6610:  20.2\n",
      "Average loss at step 6620:   9.2\n",
      "Average loss at step 6630:  12.0\n",
      "Average loss at step 6640:   6.7\n",
      "Average loss at step 6650:  17.1\n",
      "Average loss at step 6660:   8.7\n",
      "Average loss at step 6670:   4.3\n",
      "Average loss at step 6680:   7.3\n",
      "Average loss at step 6690:   8.5\n",
      "Average loss at step 6700:   8.0\n",
      "Average loss at step 6710:  10.7\n",
      "Average loss at step 6720:   9.3\n",
      "Average loss at step 6730:   8.6\n",
      "Average loss at step 6740:  14.5\n",
      "Average loss at step 6750:   9.8\n",
      "Average loss at step 6760:  12.7\n",
      "Average loss at step 6770:  11.1\n",
      "Average loss at step 6780:  12.6\n",
      "Average loss at step 6790:  15.2\n",
      "Average loss at step 6800:  13.9\n",
      "Average loss at step 6810:  23.1\n",
      "Average loss at step 6820:  14.1\n",
      "Average loss at step 6830:  18.6\n",
      "Average loss at step 6840:  14.9\n",
      "Average loss at step 6850:   6.7\n",
      "Average loss at step 6860:  14.7\n",
      "Average loss at step 6870:  12.3\n",
      "Average loss at step 6880:  14.6\n",
      "Average loss at step 6890:   3.1\n",
      "Average loss at step 6900:  21.4\n",
      "Average loss at step 6910:   6.5\n",
      "Average loss at step 6920:  11.4\n",
      "Average loss at step 6930:  18.8\n",
      "Average loss at step 6940:  11.4\n",
      "Average loss at step 6950:  11.3\n",
      "Average loss at step 6960:  13.6\n",
      "Average loss at step 6970:  12.8\n",
      "Average loss at step 6980:   4.2\n",
      "Average loss at step 6990:   8.4\n",
      "Average loss at step 7000:  10.0\n",
      "Average loss at step 7010:   7.9\n",
      "Average loss at step 7020:   9.2\n",
      "Average loss at step 7030:  14.2\n",
      "Average loss at step 7040:  21.7\n",
      "Average loss at step 7050:   7.8\n",
      "Average loss at step 7060:   9.5\n",
      "Average loss at step 7070:  13.4\n",
      "Average loss at step 7080:  19.0\n",
      "Average loss at step 7090:   4.6\n",
      "Average loss at step 7100:   8.3\n",
      "Average loss at step 7110:  10.3\n",
      "Average loss at step 7120:   5.8\n",
      "Average loss at step 7130:   6.0\n",
      "Average loss at step 7140:   8.9\n",
      "Average loss at step 7150:   9.7\n",
      "Average loss at step 7160:   7.6\n",
      "Average loss at step 7170:   9.5\n",
      "Average loss at step 7180:   8.4\n",
      "Average loss at step 7190:  13.7\n",
      "Average loss at step 7200:  13.8\n",
      "Average loss at step 7210:  11.5\n",
      "Average loss at step 7220:  15.7\n",
      "Average loss at step 7230:  10.2\n",
      "Average loss at step 7240:   8.6\n",
      "Average loss at step 7250:  19.8\n",
      "Average loss at step 7260:  16.4\n",
      "Average loss at step 7270:  24.9\n",
      "Average loss at step 7280:  18.7\n",
      "Average loss at step 7290:  11.1\n",
      "Average loss at step 7300:   5.0\n",
      "Average loss at step 7310:  13.0\n",
      "Average loss at step 7320:   6.0\n",
      "Average loss at step 7330:   2.4\n",
      "Average loss at step 7340:   5.5\n",
      "Average loss at step 7350:   5.4\n",
      "Average loss at step 7360:  10.5\n",
      "Average loss at step 7370:   2.3\n",
      "Average loss at step 7380:  22.0\n",
      "Average loss at step 7390:   8.3\n",
      "Average loss at step 7400:   9.0\n",
      "Average loss at step 7410:   8.8\n",
      "Average loss at step 7420:   6.1\n",
      "Average loss at step 7430:   9.7\n",
      "Average loss at step 7440:   9.4\n",
      "Average loss at step 7450:  17.0\n",
      "Average loss at step 7460:  10.1\n",
      "Average loss at step 7470:   6.2\n",
      "Average loss at step 7480:   7.0\n",
      "Average loss at step 7490:   7.8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss at step 7500:   8.4\n",
      "Average loss at step 7510:   3.7\n",
      "Average loss at step 7520:  10.9\n",
      "Average loss at step 7530:   8.3\n",
      "Average loss at step 7540:   4.7\n",
      "Average loss at step 7550:   7.0\n",
      "Average loss at step 7560:  10.6\n",
      "Average loss at step 7570:   3.9\n",
      "Average loss at step 7580:  18.5\n",
      "Average loss at step 7590:  10.6\n",
      "Average loss at step 7600:  23.9\n",
      "Average loss at step 7610:  13.9\n",
      "Average loss at step 7620:   5.9\n",
      "Average loss at step 7630:  14.9\n",
      "Average loss at step 7640:   4.3\n",
      "Average loss at step 7650:  11.6\n",
      "Average loss at step 7660:   6.5\n",
      "Average loss at step 7670:  20.5\n",
      "Average loss at step 7680:   7.9\n",
      "Average loss at step 7690:   8.4\n",
      "Average loss at step 7700:   2.5\n",
      "Average loss at step 7710:  10.3\n",
      "Average loss at step 7720:  13.0\n",
      "Average loss at step 7730:  15.2\n",
      "Average loss at step 7740:  18.0\n",
      "Average loss at step 7750:   6.4\n",
      "Average loss at step 7760:  18.7\n",
      "Average loss at step 7770:  11.0\n",
      "Average loss at step 7780:   9.5\n",
      "Average loss at step 7790:  15.1\n",
      "Average loss at step 7800:  12.8\n",
      "Average loss at step 7810:  11.2\n",
      "Average loss at step 7820:   5.2\n",
      "Average loss at step 7830:  10.1\n",
      "Average loss at step 7840:   9.0\n",
      "Average loss at step 7850:  11.4\n",
      "Average loss at step 7860:  12.5\n",
      "Average loss at step 7870:   9.9\n",
      "Average loss at step 7880:  11.3\n",
      "Average loss at step 7890:   2.1\n",
      "Average loss at step 7900:   8.8\n",
      "Average loss at step 7910:  10.2\n",
      "Average loss at step 7920:  12.5\n",
      "Average loss at step 7930:   4.2\n",
      "Average loss at step 7940:   7.6\n",
      "Average loss at step 7950:  10.7\n",
      "Average loss at step 7960:  10.2\n",
      "Average loss at step 7970:  18.6\n",
      "Average loss at step 7980:   7.0\n",
      "Average loss at step 7990:   2.9\n",
      "Average loss at step 8000:   7.3\n",
      "Average loss at step 8010:   9.4\n",
      "Average loss at step 8020:  14.5\n",
      "Average loss at step 8030:   8.0\n",
      "Average loss at step 8040:   8.8\n",
      "Average loss at step 8050:  13.1\n",
      "Average loss at step 8060:   6.2\n",
      "Average loss at step 8070:   9.6\n",
      "Average loss at step 8080:  17.4\n",
      "Average loss at step 8090:   9.7\n",
      "Average loss at step 8100:   7.7\n",
      "Average loss at step 8110:  10.9\n",
      "Average loss at step 8120:  14.8\n",
      "Average loss at step 8130:  12.1\n",
      "Average loss at step 8140:   6.9\n",
      "Average loss at step 8150:   7.6\n",
      "Average loss at step 8160:   6.6\n",
      "Average loss at step 8170:   4.5\n",
      "Average loss at step 8180:   7.4\n",
      "Average loss at step 8190:   5.7\n",
      "Average loss at step 8200:  13.5\n",
      "Average loss at step 8210:   5.1\n",
      "Average loss at step 8220:   6.5\n",
      "Average loss at step 8230:   8.6\n",
      "Average loss at step 8240:   9.5\n",
      "Average loss at step 8250:   5.0\n",
      "Average loss at step 8260:   8.9\n",
      "Average loss at step 8270:   6.7\n",
      "Average loss at step 8280:   8.2\n",
      "Average loss at step 8290:   5.0\n",
      "Average loss at step 8300:   7.3\n",
      "Average loss at step 8310:  10.0\n",
      "Average loss at step 8320:   8.0\n",
      "Average loss at step 8330:   0.9\n",
      "Average loss at step 8340:   9.5\n",
      "Average loss at step 8350:  11.0\n",
      "Average loss at step 8360:   4.0\n",
      "Average loss at step 8370:   5.1\n",
      "Average loss at step 8380:   8.8\n",
      "Average loss at step 8390:  12.8\n",
      "Average loss at step 8400:   8.9\n",
      "Average loss at step 8410:  13.1\n",
      "Average loss at step 8420:  17.4\n",
      "Average loss at step 8430:   7.6\n",
      "Average loss at step 8440:   7.1\n",
      "Average loss at step 8450:   6.4\n",
      "Average loss at step 8460:   8.9\n",
      "Average loss at step 8470:  17.0\n",
      "Average loss at step 8480:   9.0\n",
      "Average loss at step 8490:   9.7\n",
      "Average loss at step 8500:   4.8\n",
      "Average loss at step 8510:  11.6\n",
      "Average loss at step 8520:   7.3\n",
      "Average loss at step 8530:  13.0\n",
      "Average loss at step 8540:   3.6\n",
      "Average loss at step 8550:   4.2\n",
      "Average loss at step 8560:   8.2\n",
      "Average loss at step 8570:  13.7\n",
      "Average loss at step 8580:   2.8\n",
      "Average loss at step 8590:   6.7\n",
      "Average loss at step 8600:   6.2\n",
      "Average loss at step 8610:  13.1\n",
      "Average loss at step 8620:   3.3\n",
      "Average loss at step 8630:   8.4\n",
      "Average loss at step 8640:   9.5\n",
      "Average loss at step 8650:   7.0\n",
      "Average loss at step 8660:  14.6\n",
      "Average loss at step 8670:  10.6\n",
      "Average loss at step 8680:   6.8\n",
      "Average loss at step 8690:   9.4\n",
      "Average loss at step 8700:   9.4\n",
      "Average loss at step 8710:   8.5\n",
      "Average loss at step 8720:   1.8\n",
      "Average loss at step 8730:   9.6\n",
      "Average loss at step 8740:   6.7\n",
      "Average loss at step 8750:  10.0\n",
      "Average loss at step 8760:  20.1\n",
      "Average loss at step 8770:   5.8\n",
      "Average loss at step 8780:   9.0\n",
      "Average loss at step 8790:   5.5\n",
      "Average loss at step 8800:   7.5\n",
      "Average loss at step 8810:   3.4\n",
      "Average loss at step 8820:  10.2\n",
      "Average loss at step 8830:   8.3\n",
      "Average loss at step 8840:   3.2\n",
      "Average loss at step 8850:   7.4\n",
      "Average loss at step 8860:  10.5\n",
      "Average loss at step 8870:   5.9\n",
      "Average loss at step 8880:   3.3\n",
      "Average loss at step 8890:   7.5\n",
      "Average loss at step 8900:   6.8\n",
      "Average loss at step 8910:   6.5\n",
      "Average loss at step 8920:  10.2\n",
      "Average loss at step 8930:  10.3\n",
      "Average loss at step 8940:  12.6\n",
      "Average loss at step 8950:  10.0\n",
      "Average loss at step 8960:   9.6\n",
      "Average loss at step 8970:  10.3\n",
      "Average loss at step 8980:  13.4\n",
      "Average loss at step 8990:  10.2\n",
      "Average loss at step 9000:   4.3\n",
      "Average loss at step 9010:  22.4\n",
      "Average loss at step 9020:   3.0\n",
      "Average loss at step 9030:   6.7\n",
      "Average loss at step 9040:   9.8\n",
      "Average loss at step 9050:   6.7\n",
      "Average loss at step 9060:   1.4\n",
      "Average loss at step 9070:   6.0\n",
      "Average loss at step 9080:   9.7\n",
      "Average loss at step 9090:   1.6\n",
      "Average loss at step 9100:   8.4\n",
      "Average loss at step 9110:  10.5\n",
      "Average loss at step 9120:   9.6\n",
      "Average loss at step 9130:   8.2\n",
      "Average loss at step 9140:   6.4\n",
      "Average loss at step 9150:  10.8\n",
      "Average loss at step 9160:   4.7\n",
      "Average loss at step 9170:   5.5\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-b50ea7d6f280>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mX_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmnist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         _, loss_batch = sess.run([optimizer, loss], \n\u001b[0;32m---> 21\u001b[0;31m                                 feed_dict={X: X_batch, Y:Y_batch, dropout: DROPOUT}) \n\u001b[0m\u001b[1;32m     22\u001b[0m         \u001b[0mtotal_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss_batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mSKIP_STEP\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/postCampus/MOOC/2017_Stanford_Tensorflow/project/venv/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    776\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    777\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 778\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    779\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    780\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/postCampus/MOOC/2017_Stanford_Tensorflow/project/venv/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    980\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    981\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 982\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    983\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    984\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/postCampus/MOOC/2017_Stanford_Tensorflow/project/venv/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1030\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1031\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1032\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1033\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1034\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m~/Documents/postCampus/MOOC/2017_Stanford_Tensorflow/project/venv/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1037\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1040\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/postCampus/MOOC/2017_Stanford_Tensorflow/project/venv/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1019\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1020\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1021\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1022\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1023\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    saver = tf.train.Saver()\n",
    "    # to visualize using TensorBoard\n",
    "    writer = tf.summary.FileWriter('./my_graph/mnist', sess.graph)\n",
    "    ##### You have to create folders to store checkpoints\n",
    "    ckpt = tf.train.get_checkpoint_state(os.path.dirname('checkpoints/convnet_mnist/checkpoint'))\n",
    "    # if that checkpoint exists, restore from checkpoint\n",
    "    if ckpt and ckpt.model_checkpoint_path:\n",
    "        saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "    \n",
    "    initial_step = global_step.eval()\n",
    "\n",
    "    start_time = time.time()\n",
    "    n_batches = int(mnist.train.num_examples / BATCH_SIZE)\n",
    "\n",
    "    total_loss = 0.0\n",
    "    for index in range(initial_step, n_batches * N_EPOCHS): # train the model n_epochs times\n",
    "        X_batch, Y_batch = mnist.train.next_batch(BATCH_SIZE)\n",
    "        _, loss_batch = sess.run([optimizer, loss], \n",
    "                                feed_dict={X: X_batch, Y:Y_batch, dropout: DROPOUT}) \n",
    "        total_loss += loss_batch\n",
    "        if (index + 1) % SKIP_STEP == 0:\n",
    "            print('Average loss at step {}: {:5.1f}'.format(index + 1, total_loss / SKIP_STEP))\n",
    "            total_loss = 0.0\n",
    "            saver.save(sess, 'checkpoints/convnet_mnist/mnist-convnet', index)\n",
    "    \n",
    "    print(\"Optimization Finished!\") # should be around 0.35 after 25 epochs\n",
    "    print(\"Total time: {0} seconds\".format(time.time() - start_time))\n",
    "    \n",
    "    # test the model\n",
    "    n_batches = int(mnist.test.num_examples/BATCH_SIZE)\n",
    "    total_correct_preds = 0\n",
    "    for i in range(n_batches):\n",
    "        X_batch, Y_batch = mnist.test.next_batch(BATCH_SIZE)\n",
    "        _, loss_batch, logits_batch = sess.run([optimizer, loss, logits], \n",
    "                                        feed_dict={X: X_batch, Y:Y_batch, dropout: DROPOUT}) \n",
    "        preds = tf.nn.softmax(logits_batch)\n",
    "        correct_preds = tf.equal(tf.argmax(preds, 1), tf.argmax(Y_batch, 1))\n",
    "        accuracy = tf.reduce_sum(tf.cast(correct_preds, tf.float32))\n",
    "        total_correct_preds += sess.run(accuracy)   \n",
    "    \n",
    "    print(\"Accuracy {0}\".format(total_correct_preds/mnist.test.num_examples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
