{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Guest Lecture: Lessons Learned Building the Neural GPU\n",
    "*≈Åukasz Kaiser*  \n",
    "build NNs to learn algorithms\n",
    "\n",
    "## 1. Feed-forward vs. Recurrent Neural Network\n",
    "- Feed-forward neural network\n",
    "    - fixed-size input\n",
    "    - fixed-size output\n",
    "    - number of parameters depends on these sizes\n",
    "    - limited computational power\n",
    "- RNN\n",
    "    - variable-size input\n",
    "    - variable-size output\n",
    "    - number of parameters depends on memory size\n",
    "    - computational power limited by this memory size\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Neural Algorithm Learning\n",
    "### 2.1 Previous work on neural algorithm learning\n",
    "- RNN and LSTM-based, generalizes a bit with attention\n",
    "- Data-structure based (stack RNN, memory nets)\n",
    "- Neural Turing Machines\n",
    "\n",
    "### 2.2 problem with RNN\n",
    "- feasible only if testing length ~ training\n",
    "- with higher length we need to increase network capacity (number of parameters)\n",
    "- Fail to capture algorithmic patterns, even in principle\n",
    "\n",
    "### 2.3 problem with NTM\n",
    "- Look close, your computer does not have a tape!\n",
    "- Very sequential in nature, hard to train on hard problems\n",
    "- Example problem: long multiplication\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Neural Computation\n",
    "### 3.1 RNN vs NTM\n",
    "- Recurrent Neural Networks: share weights across time steps but use fixed-size memory vectors\n",
    "    - Time: $O(f(n))$\n",
    "    - Space: $O(1)$\n",
    "- Neural Turing Machines:  work with variable-size memory and so can do general computation, but are sequential\n",
    "    - Time: $O(f(n))$\n",
    "    - Space: $O(g(n))$\n",
    "    \n",
    "![RNNvsNTM](figures/12_01.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Arbitrary Size Ops\n",
    "Neural computation requires operators that use a **fixed number of parameters** but operate on **memory of arbitrary size**.  \n",
    "Candidate operators:\n",
    "- Attention (Neural Turing Machine)  \n",
    "- Stack, (De)Queue, Lists\n",
    "- ... other memory structures ...\n",
    "- **Convolutions!**: act like a **neural GPU**\n",
    "    - Attention is a softmax, effectively ~1 memory item / step.\n",
    "    - Similarly a stack and other task-specific memory structures\n",
    "    - Convolutions affect <u>all memory items in each step</u>\n",
    "    - Convolutions are already implemented and optimized\n",
    "    - To train well we need to use LSTM-style gates: <u>CGRN</u> (Convolutional Gated Recurrent Networks)\n",
    "![attention](figures/12_02.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Neural GPU\n",
    "**Convolutional Gated Recurrent Networks (CGRNs)** perform many parallel operations in each step, akin\n",
    "to a neural GPU and in contrast to the sequential nature of Neural Turing Machines.  \n",
    "### 4.1 CGRU definition\n",
    "(similar to GRU replacing linear by convolution)  \n",
    "$$s_{t+1}=g \\cdot s_t + (1-g) \\cdot c$$  \n",
    "where:  \n",
    "$$g=sigmoid(conv(s_t, K_g))$$  \n",
    "$$c=tanh(conv(s_t \\cdot r, K_c))$$  \n",
    "$$r=sigmoid(conv(s_t, K_r))$$\n",
    "\n",
    "### 4.2 Computational power\n",
    "- Small number of parameters (3 kernels: $K_g$, $K_c$, $K_r$)\n",
    "- Can simulate computation of **cellular automata**\n",
    "- With memory of size $n$ can do $n$ local operations / step\n",
    "- E.g., can do long multiplication in $O(n)$ steps  \n",
    "![CGRU](figures/12_03.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Results\n",
    "A n-step n-size memory CGRN achieves 100x generalization on a number of tasks such as reversing sequences, long addition, or even non-linear-time long multiplication.  \n",
    "![results](figures/12_04.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Tricks of the Trade\n",
    "#### Curriculum learning\n",
    "- Start training on small lengths, increase when learned  \n",
    "\n",
    "#### Parameter sharing relaxation\n",
    "- Allow to do different things in different time-steps first\n",
    "- Not needed now with bigger models and orthogonal init  \n",
    "\n",
    "#### Dropout on recurrent connections\n",
    "- Randomly set 10% of state vectors to 0 in each step\n",
    "- Interestingly, this is key for good length generalization  \n",
    "\n",
    "#### Noise added to gradients\n",
    "- Add small gaussian noise to gradients in each training step  \n",
    "\n",
    "#### Gate cutoff (saturation)\n",
    "- Instead of $sigmoid(x)$ use $[1.2sigmoid(x) - 0.1]_{[0,1]}$  \n",
    "\n",
    "#### Tune parameters\n",
    "- A lot of GPUs running for a few months; or: better method!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5 How to code\n",
    "#### Is the graph static or dynamic?\n",
    "- Dynamic ops were not exposed in first TF release, tricky\n",
    "- Static graph requires bucketing and takes long to build\n",
    "- **Focus**: why conditionals are tricky: batches and lambdas  \n",
    "\n",
    "#### How do we do bucketing?\n",
    "- Sparse or dense buckets? Masks are bug-prone.\n",
    "- How do we bucket training data? Feed or queues?\n",
    "- If queues, how to do curriculum? How to not starve?  \n",
    "\n",
    "#### How do we write layers?\n",
    "- Is there a canonical way to define new functions?\n",
    "- Frameworks: Keras vs Slim (OO vs functional)\n",
    "- Unification in `tf.layers`: callable objects save scope\n",
    "- Example: weight-normalization through custom_getter  \n",
    "\n",
    "#### How do we organize experiments?\n",
    "- Use `tf.learn`, Estimator, Experiment.\n",
    "- How to register models and problems? Save runs?\n",
    "- Hyper-parameters manual or tuned with ranges?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
