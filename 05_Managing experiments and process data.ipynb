{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lecture 5: Managing experiments and process data\n",
    "### 0. Overview\n",
    "- tf.train.Saver() class\n",
    "- visualize summary statistics during training\n",
    "- random seed (TF) and random state (NP)\n",
    "- reading data in TF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. tf.trainSaver()\n",
    "- save the graph's variables (model parameters) in binary files\n",
    "- enable restoration/retainment of model after a certain number of steps\n",
    "- **checkpoint**: the step at which you save your graph's variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    tf.train.Saver.save(sess, save_path, global_step=None, latest_filename=None, meta_graph_suffix='meta', write_meta_graph=True, write_state=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- example: save variables of the graph after every 1000 training steps\n",
    "\n",
    "        # define model\n",
    "\n",
    "        # create a saver object\n",
    "        saver = tf.train.Saver()\n",
    "\n",
    "        # launch a session to compute the graph\n",
    "        with tf.Session as sess:\n",
    "            # actual training loop\n",
    "            for step in range(training_steps):\n",
    "                sess.run([optimiser])\n",
    "\n",
    "                if (step + 1) % 1000 ==0:\n",
    "                    saver.save(sess, 'checkpoint_directory/model_name',\n",
    "                              global_step=model.global_step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- helpful to append the # of training steps the model has gone through in a variable `global_step` (initialize to be 0, NOT trainable)\n",
    "\n",
    "        self.global_step = tf.Variable(0, dtype=tf.int32, trainable=False, name='global_step')\n",
    "        \n",
    "        # pass global_step as a parameter to the optimizer\n",
    "        # so it is icremented by one with each training step\n",
    "        self.optimizer = tf.train.GradientDescentOptimizer(self.lr).minimize(self.loss, global_step=self.global_step)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- training loop for word2vec:\n",
    "\n",
    "        self.global_step = tf.Variable(0, dtype=tf.int32, trainable=False, name='global_step')\n",
    "\n",
    "        self.optimizer = tf.train.GradientDescentOptimizer(self.lr).minimize(self.loss, global_step=self.global_step)\n",
    "\n",
    "        saver = tf.train.Saver() # defaults to saving all variables\n",
    "\n",
    "        with tf.Session() as sess:\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "\n",
    "            average_loss = 0.0\n",
    "            writer = tf.summary.FileWriter('./improverd_graph', sess.graph)\n",
    "            for index in xrange(num_train_steps):\n",
    "                batch = batch_gen.next()\n",
    "                loss_batch, _ = sess.run([model.loss, model.optimizer],\n",
    "                                        feed_dict={model.center_words: batch[0],\n",
    "                                                   model.target_words: batch[1]})\n",
    "                average_loss += loss_batch\n",
    "                if (index + 1) % 1000 ==0:\n",
    "                    saver.save(sess, 'checkpoints/skip-gram', global_step=model.global_step)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- restore **variables** (not the entire graph, need to create graph ourselves):\n",
    "    \n",
    "        # check if there is a checkpoint\n",
    "        ckpt = tf.train.get_checkpoint_state(os.path.dirname('checkpoints/checkpoint'))\n",
    "        \n",
    "        if ckpt and ckpt.model_checkpoint_path:\n",
    "            # restore variables\n",
    "            saver.restore(sess, 'checkpoints/skip-gram-10000')\n",
    "            \n",
    "    - stores all variables by default (recommended)\n",
    "    - store selected variables:\n",
    "\n",
    "            v1 = tf.Variable(..., name='v1')\n",
    "            v2 = tf.Variable(..., name='v2')\n",
    "\n",
    "            # pass the variables as a dict\n",
    "            saver = tf.train.Saver({'v1': v1, 'v2': v2})\n",
    "\n",
    "            # pass the variables as a list\n",
    "            saver = tf.train.Saver([v1, v2])\n",
    "\n",
    "            # passing a list is squivalent to passing a dict with the variable op names # as keys\n",
    "            saver = tf.train.Saver({v.op.name: v for v in [v1, v2]})\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.  visualize summary statistics during training\n",
    "#### 2.1 some popular statistics to visualize:\n",
    "- loss\n",
    "- average loss\n",
    "- accuracy\n",
    "\n",
    "#### 2.2 walkthrough\n",
    "- namescope to hold summary ops\n",
    "\n",
    "        def _create_summaries(self):\n",
    "            with tf.name_scope(\"summaries\"):\n",
    "                tf.summary.scalar(\"loss\", self.loss)\n",
    "                tf.summary.scalar(\"accuracy\", self.accuracy)\n",
    "                tf.summary.histogram(\"histogram loss\", self.loss)\n",
    "\n",
    "                # because you have several summaries, we should merge them all\n",
    "                # into one op to make it easier to manage\n",
    "                self.summary_op = tf.summary.merge_all()\n",
    "\n",
    "- because it's an op, have to execute it:\n",
    "\n",
    "        loss_batch, _, summary = sess.run([model.loss, model.optimizer, model.summary_op],\n",
    "                                          feed_dict=feed_dict)\n",
    "                                          \n",
    "- write the summary to file (using the same FileWriter object we created to visual our graph)\n",
    "\n",
    "        writer.add_summary(summary, global_step=step)\n",
    "        \n",
    "- run TensorBoard and go to http://localhost:6006/, check Scalars page\n",
    "- can compare the progress made with different optimizers or different parameters\n",
    "- visualize the statistics as images\n",
    "\n",
    "        tf.summary.image(name, tensor, max_outputs=3, collections=None)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Control randomization\n",
    "TensorFlow doesn’t allow to you to get random state the way numpy does.\n",
    "\n",
    "#### 3.1 2 ways to get stable results in randomization:\n",
    "- Set random seed at operation level\n",
    "        \n",
    "        my_var = tf.Variable(tf.truncated_normal((-1.0,1.0), stddev=0.1, seed=0))\n",
    "        \n",
    "   **session keeps track of random state**, each new session will start the random state all over again\n",
    "   \n",
    "        c = tf . random_uniform ([], - 10 , 10 , seed = 2)\n",
    "        with tf . Session () as sess:\n",
    "        print sess . run ( c) # >> 3.57493\n",
    "        print sess . run ( c) # >> -5.97319\n",
    "\n",
    "\n",
    "        c = tf . random_uniform ([], - 10 , 10 , seed = 2)\n",
    "        with tf . Session () as sess:\n",
    "        print sess . run ( c) # >> 3.57493\n",
    "        with tf . Session () as sess:\n",
    "        print sess . run ( c) # >> 3.57493\n",
    "\n",
    "    with operation-level random seed, each op keeps its own seed\n",
    "    \n",
    "        c = tf.random_uniform([], -10, 10, seed=2)\n",
    "        d = tf.random_uniform([], -10, 10, seed=2)\n",
    "        \n",
    "        with tf.Session as sess:\n",
    "            print(sess.run(c)) # >> 3.57493\n",
    "            print(sess.run(d)) # >> 3.57493\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Set random seed at graph level with tf.Graph.seed  \n",
    "If you don’t care about the randomization for each op inside the graph, but just want to be able\n",
    "to replicate result on another graph (so that other people can replicate your results on their own\n",
    "graph), you can use tf.set_random_seed instead.  \n",
    "For example, you have two models a.py and b.py that have identical code:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.set_random_seed(2)\n",
    "c = tf.random_uniform([], -10 , 10)\n",
    "d = tf.random_uniform([], -10 , 10)\n",
    "with tf.Session() as sess:\n",
    "print sess. run (c)\n",
    "print sess. run (d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Without graph level seed, running python a.py and b.py will return 2 completely different results,\n",
    "but with tf.set_random_seed, you will get two identical results:\n",
    "\n",
    "    $ python a . py\n",
    "    >> - 4.00752\n",
    "    >> - 2.98339\n",
    "    $ python b . py\n",
    "    >> - 4.00752\n",
    "    >> - 2.98339"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Reading data in TensorFlow\n",
    "#### 4.1 through feed_dict\n",
    "Feed_dict will first send data from the storage system to the client, and then\n",
    "from client to the worker process. This will cause the data to slow down, especially if the client is\n",
    "on a different machine from the worker process.\n",
    "\n",
    "#### 4.2 through readers\n",
    "Allow us to load data directly into the worker process\n",
    "- `tf.TextLineReader`: Outputs the lines of a file delimited by newlines.  \n",
    "e.g. text files, CSV files\n",
    "- `tf.FixedLengthRecordReader`: Outputs the entire file when all files have same fixed lengths  \n",
    "e.g. each MNIST file has 28 x 28 pixels , CIFAR-10 32 x 32 x 3\n",
    "- `tf.WholeFileReader`: Outputs the entire file content\n",
    "- `tf.TFRecordReader`: Reads samples from TensorFlow's own binary format (TFRecord)\n",
    "- `tf.ReaderBase`: Allows you to create your own readers  \n",
    "more in [Lecture 9](09_Input Pipeline.ipynb)\n",
    "\n",
    "#### 4.3 load data using constants\n",
    "only use this if want graph to be seriously bloated and un-runnable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
