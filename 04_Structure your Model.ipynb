{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 4: Structure your model\n",
    "## 0. Overview\n",
    "- Overall structure of a model in TensorFlow\n",
    "- word2vec\n",
    "- Name scope\n",
    "- Embedding visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Overall structure of a model in TensorFlow\n",
    "### Phase 1: Assemble graph\n",
    "1. Define placeholders for input and output\n",
    "2. Define the weights\n",
    "3. Define the inference model\n",
    "4. Define loss function\n",
    "5. Define optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Phase 2: Compute\n",
    "![training loop](figures/04_01.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. word2vec\n",
    "### 2.1 about word2vec\n",
    "[ Simple Word Vector Representations](http://web.stanford.edu/class/cs224n/lectures/cs224n-2017-lecture2.pdf)\n",
    "- main idea: Predict between every word and its context words\n",
    "- algorithms\n",
    "    1. Continuous Bag of Words (CBOW): predict target word from bag-of-words context\n",
    "        - e.g. sentence \"The brown fox jumps.\": predict \"brown\" from \"the\", \"fox\", \"jumps\"\n",
    "        - treats entire context as 1 observation, therefore smoothes over a lot of the distributional info\n",
    "        - useful for smaller datasets\n",
    "    2. Skip-grams (SG): predict context words given context (position independent)\n",
    "        - e.g. sentence \"The brown fox jumps.\": predict \"the\", \"fox\", \"jumps\" from \"brown\"\n",
    "        - treats each context-target pair as a new observation\n",
    "        - does better for larger datasets\n",
    "- training methods (moderatiely efficient)  \n",
    "    0. training is aimed to: \n",
    "        - minimize the cross-entropy loss of our model for every word $w$ in the training set (info theory)\n",
    "        - i.e. minimizing the negative log likelihood of the correct class (probablistic interpretation): MLE or MAP (if use the regularization term $R(W)$ in the full loss function)\n",
    "        - ([explanation](http://cs231n.github.io/linear-classify/#softmax-classifier))\n",
    "    \n",
    "    1. hierarchical softmax   \n",
    "        - softmax-based approach\n",
    "        - structure the softmax as a binary tree\n",
    "        - **softmax vs SVM**  \n",
    "    ![softmaxvssvm](figures/04_02.png)\n",
    "    **naiive softmax**: normalization factor is too computationally expensive  \n",
    "    2. negative sampling  \n",
    "        -  sampling-based approach (others: importance sampling, target sampling)\n",
    "        -  a simplified model of  **Noise Contrastive Estimation (NCE)**: makes certain assumption about the number of noise samples to generate (k) and the distribution of noise samples (Q) (negative sampling assumes that kQ(w) = 1)\n",
    "        - useful for the learning word embeddings, but doesn’t have the theoretical guarantee that its derivative tends towards the gradient of the softmax function, which makes it not so useful for language modelling\n",
    "        - [Sebastian Rudder’s “On word embeddings - Part 2: Approximating the Softmax” ](http://sebastianruder.com/word-embeddings-softmax/index.html)  \n",
    "        - [Chris Dyer’s “Notes on Noise Contrastive Estimation and Negative Sampling”)](http://demo.clab.cs.cmu.edu/cdyer/nce_notes.pdf)\n",
    "    3. NCE (used in the example)\n",
    "    \n",
    "- [word2vec simple tutorial: skip-gram model](http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/)\n",
    "- [google code](https://code.google.com/archive/p/word2vec/)\n",
    "\n",
    "### 2.2 about the dataset\n",
    "text8: the first 100 MB of cleaned text of the English Wikipedia dump on Mar. 3, 2006 (whose link is no longer available)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 3. create word2vec, skip-gram model\n",
    "### Phase 1: Assemble the graph\n",
    "#### 1. Define placeholders for input and output\n",
    "- input: center word\n",
    "- output: target (context) word\n",
    "-  Instead of using one-hot vectors, use the **index** of those words directly (scalar placeholder with shape\n",
    "`[BATCH_SIZE]` )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    center_words = tf.placeholder(tf.int32, shape = [BATCH_SIZE])\n",
    "    target_words = tf.placeholder(tf.int32, shape = [BATCH_SIZE])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Define the weight (in this case, embedding matrix)\n",
    "- each row corresponds to the representation vector of one word (with size `EMBED_SIZE`)\n",
    "- shape of embedding matrix: `[VOCAB_SIZE, EMBED_SIZE]`\n",
    "- initialize the embedding matrix to value from a random distribution (e.g. uniform distribution)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    embed_matrix = tf.Variable(tf.random_uniform([VOCAB_SIZE, EMBED_SIZE], -1.0, 1.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Inference (compute the forward path of the graph)\n",
    "- goal: to get the vector representations of words in our dictionary\n",
    "- to get the representation of all the center words in the batch, we get the slice of all corresponding rows in the embedding matrix ( `tf.nn.embedding_lookup(params, ids, partition_strategy='mod', name=None,\n",
    "validate_indices=True, max_norm=None)` )\n",
    "- `tf.nn.embedding_lookup()` is useful when it comes to matrix multiplication with one-hot vectors because it saves us from doing a bunch of unnecessary computation that will return 0 anyway\n",
    "![matrix multiplication of one-hot](figures/04_03.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    embed = tf.nn.embedding_lookup(embed_matrix, center_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Define the loss function (NCE)\n",
    "- [nce_loss source code](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/nn_impl.py)  \n",
    "- **note: the third argument is actually inputs, and the fourth is labels**  \n",
    "- need weights and biases for the hidden layer to calculate NCE loss, then define loss\n",
    "\n",
    "        tf.nn.nce_loss(weights, biases, labels, inputs, num_sampled, num_classes, num_true=1, sampled_values=None, remove_accidental_hits=False, partition_strategy='mod',name='nce_loss')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "    nce_weight = tf.Variable(tf.truncated_normal([VOCAB_SIZE, EMBED_SIZE], stddev=1.0 / EMBED_SIZE ** 0.5))\n",
    "    nce_bias = tf.Variable(tf.zeros([VOCAB_SIZE]))\n",
    "    loss = tf.reduce_mean(tf.nn.nce_loss(weights=nce_weight,\n",
    "                                         bias=nce_bias,\n",
    "                                         labels=target_words,\n",
    "                                         inputs=embed,\n",
    "                                         num_sampled=NUM_SAMPLED,\n",
    "                                         num_classes=VOCAB_SIZE))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Define optimizer (gradient descent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    optimiser = tf.train.GradientDescentOptimiser(LEARNING_RATE).minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Phase 2: Execute the computation\n",
    "1. create a session\n",
    "2. feed inputs and outputs into the placeholders using `feed_dict`\n",
    "3. run the optimizer to minimize the loss\n",
    "4. fetch the loss value to report back"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    with tf.Session() as sess:\n",
    "         sess.run(tf.global_variables_initializer())\n",
    "\n",
    "         average_loss = 0.0\n",
    "         for index in xrange(NUM_TRAIN_STEPS):  \n",
    "         # range() returns list object, xrange() returns xrange object\n",
    "         # latter is better for generating large number of indeces\n",
    "             batch = batch_gen.next()\n",
    "             loss_batch, _ = sess.run([loss, optimizer],\n",
    "                                     feed_dict={center_words: batch[0], target_words: batch[1]})\n",
    "             average_loss += loss_batch\n",
    "             if (index + 1) % 2000 == 0:\n",
    "                 print('Average loss at step {}: {:5.1f}').format(index + 1, average_loss / (index + 1))\n",
    "\n",
    "                 # w.pf -> w=max width of the entire number, p=precision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [code for full implementation](https://github.com/chiphuyen/tf-stanford-tutorials/blob/master/examples/04_word2vec_no_frills.py)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Name Scope\n",
    "### 4.1 a look at TensorBoard\n",
    "![tensorboad_original](figures/04_04.png)\n",
    "\n",
    "- not very readable: TensorBoard does not know which nodes are similar\n",
    "- need to group related nodes (using name scope)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    with tf.name_scope(name_of_that_scope):\n",
    "        # declare op_1\n",
    "        # declare op_2\n",
    "        # ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2 use name scope to build grouped blocks\n",
    "the graph can have 3 op blocks:\n",
    "- \"data\"\n",
    "- \"embed\": has two nodes\n",
    "    - one for tf.Variable\n",
    "    - one for if.random_uniform\n",
    "- \"NCE_LOSS\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    with tf.name_scope('data'):\n",
    "        center_words = tf.placeholder(tf.int32, shape=[BATCH_SIZE], name='center_words')\n",
    "        target_words = tf.placeholder(tf.int32, shape=[BATCH_SIZE, 1], name='target_words')\n",
    "    with tf.name_scope('embed'):\n",
    "        embed_matrix = tf.Variable(tf.random_uniform([VOCAB_SIZE, EMBED_SIZE], -1.0, 1.0), name='embed_matrix')\n",
    "    with tf.name_scope('loss'):\n",
    "        embed = tf.nn.embedding_lookup(embed_matrix, center_words, name='embed')\n",
    "        nce_weight = tf.Variable(tf.truncated_normal([VOCAB_SIZE, EMBED_SIZE],\n",
    "                                                    stddev=1.0 / math.sprt(EMBED_SIZE)),\n",
    "                                name='nce_weight')\n",
    "        nce_bias = tf.Variable(tf.zeros([VOCAB_SIZE]), name='nce_bias')\n",
    "        loss = tf.reduce_mean(tf.nn.nce_loss(weights=nce_weight,\n",
    "                                            biases=nce_bias,\n",
    "                                            labels=target_words,\n",
    "                                            inputs=embed,\n",
    "                                            num_sampled=NUM_SAMPLED,\n",
    "                                            num_classes=VOCAB_SIZE),\n",
    "                              name='loss')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.4 view grouped TensorBoard\n",
    "![tensorboad_grouped](figures/04_05.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. build model as a class\n",
    "### 5.1 why build class?\n",
    "- can't dump everything into a giant function\n",
    "- make model most easy to use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.2 getting started"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    class SkipGramModel:\n",
    "        \"\"\" Build the graph for word2vec model\"\"\"\n",
    "        def __init__(self, params): # constructor function\n",
    "            “”“ Step 1: define the placeholders for input and output \"\"\"\n",
    "            pass\n",
    "\n",
    "        def _create_placeholders(self):\n",
    "            “”“ Step 2: define weights. In word2vec, it's actually the weights that we care\n",
    "    about \"\"\"\n",
    "            pass\n",
    "\n",
    "        def _create_loss(self):\n",
    "             \"\"\" Step 3 + 4: define the inference + the loss function \"\"\"\n",
    "             pass\n",
    "\n",
    "         def _create_optimizer(self):\n",
    "             \"\"\" Step 5: define optimizer \"\"\"\n",
    "             pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Results\n",
    "#### 6.1 visualized embedding (using t-SNE)\n",
    "![tSNE](figures/04_06.png)\n",
    "\n",
    "patterns found in 3D:\n",
    "- all the number (one, two, …, zero) are grouped in a line on the bottom right\n",
    "- All the months are grouped together\n",
    "- “Do”, “does”, “did” are grouped together\n",
    "- ...\n",
    "\n",
    "#### 6.2 about t-SNE\n",
    "- [from-sne-to-tsne-to-largevis](http://bindog.github.io/blog/2016/06/04/from-sne-to-tsne-to-largevis/)\n",
    "- 2 main stages\n",
    "    - construct a probability distribution over pairs of high-dimensional objects in such a way that similar objects have a high probability of being picked, whilst dissimilar points have an extremely small probability of being picked.\n",
    "    - defines a similar probability distribution over the points in the low-dimensional map, and it minimizes the Kullback–Leibler divergence between the two distributions with respect to the locations of the points in the map\n",
    "    - note: whilst the original algorithm uses the Euclidean distance between objects as the base of its similarity metric, this should be changed as appropriate\n",
    "- example: [visualizing MNIST](http://colah.github.io/posts/2014-10-Visualizing-MNIST/)\n",
    "\n",
    "#### 6.3 visualized embedding (using PCA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    from tensorflow.contrib.tensorboard.plugins import projector\n",
    "    \n",
    "    # obtain the embedding_matrix after you’ve trained it\n",
    "    final_embed_matrix = sess.run(model.embed_matrix)\n",
    "    \n",
    "    # create a variable to hold your embeddings. It has to be a variable. Constants\n",
    "    # don’t work. You also can’t just use the embed_matrix we defined earlier for our model. Why\n",
    "    # is that so? I don’t know. I get the 500 most popular words.\n",
    "    embedding_var = tf.Variable(final_embed_matrix[:500], name='embedding')\n",
    "    sess.run(embedding_var.initializer)\n",
    "    config = projector.ProjectorConfig()\n",
    "    summary_writer = tf.summary.FileWriter(LOGDIR)\n",
    "    \n",
    "    # add embeddings to config\n",
    "    embedding = config.embeddings.add()\n",
    "    embedding.tensor_name = embedding_var.name\n",
    "    \n",
    "    # link the embeddings to their metadata file. In this case, the file that contains\n",
    "    # the 500 most popular words in our vocabulary\n",
    "    embedding.metadata_path = LOGDIR + '/vocab_500.tsv'\n",
    "    \n",
    "    # save a configuration file that TensorBoard will read during startup\n",
    "    projector.visualize_embeddings(summary_writer, config)\n",
    "    \n",
    "    # save our embedding\n",
    "    saver_embed = tf.train.Saver([embedding_var])\n",
    "    saver_embed.save(sess, LOGDIR + '/skip-gram.ckpt', 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we run our model again, then again run tensorboard. If you go to http://localhost:6006, click\n",
    "on the Embeddings tab, you’ll see all the visualization.\n",
    "\n",
    "[more resource](https://www.tensorflow.org/get_started/embedding_viz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
