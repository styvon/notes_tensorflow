{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 13: Seq2seq with Attention \n",
    "## 0. Overview\n",
    "- Seq2seq\n",
    "- Implementation keys\n",
    "- Example: Chatbot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Sequence to Sequence\n",
    "### 1.1 intro\n",
    "- The current model class of choice for most dialogue and machine translation systems\n",
    "- [Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation,  Cho et al. 2014](https://arxiv.org/pdf/1406.1078.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Structure: two RNNs\n",
    "- Encoder: maps a variable-length source sequence (input) to a fixed-length vector\n",
    "- Decoder: maps the vector representation back to a variable-length target sequence (output)   \n",
    "\n",
    "Two RNNs are trained jointly to maximize the conditional probability of the target sequence given a source sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Encoder and Decoder in TensorFlow\n",
    "#### Vanilla model\n",
    "Each input has to be encoded into a **fixed-size state vector** (the only thing to be passed to decoder)\n",
    "![vanilla](figures/13_01.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model with attention\n",
    "Decoder gets direct access to input data\n",
    "![attention](figures/13_02.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Implementation keys\n",
    "### 2.1 Bucketing\n",
    "- Avoid too much padding\n",
    "- Group sequences of similar lengths into the same buckets\n",
    "- Create a separate subgraph for each bucket\n",
    "- in theory (v1.0)\n",
    "\n",
    "        tf.contrib.training.bucket_by_sequence_length(max_length, examples, batch_size, bucket_boundaries, capacity=2 * batch_size, dynamic_pad=True)\n",
    "        \n",
    "- in practice (v0.12): [TF translate model](https://www.tensorflow.org/tutorials/seq2seq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Sampled Softmax\n",
    "- advantage: avoid the growing complexity of computing the normalization constant\n",
    "- idea: approximate the negative term of the gradient by importance sampling with a small number of samples\n",
    "    - At each step, update only the vectors associated with the correct word $w$ and with the sampled words in $V’$\n",
    "    - Once training is over, use the full target vocabulary to compute the output probability of each target word\n",
    "- [On Using Very Large Target Vocabulary for Neural Machine Translation (Jean et al., 2015)](https://arxiv.org/pdf/1412.2007.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if config.NUM_SAMPLES > 0 and config.NUM_SAMPLES < config.DEC_VOCAB:\n",
    "    weight = tf.get_variable('proj_w', [config.HIDDEN_SIZE, config.DEC_VOCAB])\n",
    "    bias = tf.get_variable('proj_b', [config.DEC_VOCAB])\n",
    "    self.output_projection = (w, b)\n",
    "    \n",
    "    def sampled_loss(inputs, labels):\n",
    "        labels = tf.reshape(labels, [-1, 1])\n",
    "        return tf.nn.sampled_softmax_loss(tf.transpose(weight), bias, inputs, labels, \n",
    "                                          config.NUM_SAMPLES, config.DEC_VOCAB)\n",
    "    self.softmax_loss_function = sampled_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Generally an underestimate of the full softmax loss\n",
    "- At inference time, compute the **full softmax** using\n",
    "\n",
    "        tf.nn.softmax(tf.matmul(inputs, tf.transpose(weight)) + bias)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Seq2seq in TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "outputs, states = basic_rnn_seq2seq(encoder_inputs, \n",
    "                                    decoder_inputs, \n",
    "                                    cell)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **encoder_inputs**: a list of tensors representing inputs to the encoder\n",
    "- **decoder_inputs**: a list of tensors representing inputs to the decoder\n",
    "- **cell**: single or multiple layer cells\n",
    "- **outputs**: a list of decoder_size tensors, each of dimension 1 x DECODE_VOCAB corresponding to the probability distribution at each time-step\n",
    "- **states**: a list of decoder_size tensors, each corresponds to the internal state of the decoder at every time-step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "outputs, states = embedding_rnn_seq2seq(encoder_inputs,\n",
    "                                        decoder_inputs,\n",
    "                                        cell,\n",
    "                                        num_encoder_symbols,\n",
    "                                        num_decoder_symbols,\n",
    "                                        embedding_size,\n",
    "                                        output_projection=None,\n",
    "                                        feed_previous=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **num_encoder_symbols** & **num_decoder_symbols**: To embed your inputs and outputs, need to specify the number of input and output tokens\n",
    "- **feed_previous** if you want to feed the previously predicted word to train, even if the model makes mistakes\n",
    "- **output_projection**: tuple of project weight and bias if use sampled softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "outputs, states = embedding_attention_seq2seq(encoder_inputs,\n",
    "                                              decoder_inputs,\n",
    "                                              cell,\n",
    "                                              num_encoder_symbols,\n",
    "                                              num_decoder_symbols,\n",
    "                                              num_heads=1,\n",
    "                                              output_projection=None,\n",
    "                                              feed_previous=False,\n",
    "                                              initial_state_attention=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "outputs, losses = model_with_buckets(encoder_inputs,\n",
    "                                     decoder_inputs,\n",
    "                                     targets,\n",
    "                                     weights,\n",
    "                                     buckets,\n",
    "                                     seq2seq,\n",
    "                                     softmax_loss_function=None,\n",
    "                                     per_example_loss=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **seq2seq**: one of the seq2seq functions defined above\n",
    "- **softmax_loss_function**: normal softmax or sampled softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. TensorFlow chatbot\n",
    "### 3.1 [Cornell Movie-Dialogs Corpus](https://www.cs.cornell.edu/~cristian/Cornell_Movie-Dialogs_Corpus.html)\n",
    "- 220,579 conversational exchanges between\n",
    "- 10,292 pairs of movie characters\n",
    "- 9,035 characters from 617 movies\n",
    "- 304,713 total utterances\n",
    "- Very well-formatted (almost perfect)\n",
    "- [*Chameleons in imagined conversations, Cristian Danescu-Niculescu-Mizil and Lillian Lee*](http://www.cs.cornell.edu/~cristian/papers/chameleons.pdf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 input & bucketing\n",
    "- input length distribution\n",
    "![inputlen](figures/13_03.png)\n",
    "- bucketing\n",
    "    - 9 buckets: \n",
    "    \n",
    "            [(6, 8), (8, 10), (10, 12), (13, 15), (16, 19), (19, 22), (23, 26), (29, 32), (39, 44)]\n",
    "            [19530, 17449, 17585, 23444, 22884, 16435, 17085, 18291, 18931]\n",
    "            \n",
    "    - 5 buckets:\n",
    "    \n",
    "            [(8, 10), (12, 14), (16, 19), (23, 26), (39, 43)] # bucket boundaries\n",
    "            [37049, 33519, 30223, 33513, 37371] # number of samples in each bucket\n",
    "\n",
    "    - 3 buckets (recommended)\n",
    "    \n",
    "            [(8, 10), (12, 14), (16, 19)]\n",
    "            [37899, 34480, 31045]\n",
    "\n",
    "#### Vocabulary tradeoff\n",
    "- Get all tokens that appear at least a number of time (twice)\n",
    "- Alternative approach: get a fixed size vocabulary\n",
    "\n",
    "#### Smaller vocabulary:\n",
    "- Has smaller loss/perplexity (but loss/perplexity isn’t everything)\n",
    "- Gives <unk> answers to questions that require personal information\n",
    "- Doesn’t give the bot’s answers much response\n",
    "- Doesn’t train much faster than big vocab using sampled softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Model\n",
    "- Seq2seq\n",
    "- Attentional decoder\n",
    "- Reverse encoder inputs\n",
    "- Bucketing\n",
    "- Sampled softmax\n",
    "- Based on the Google’s vanilla translate model (originally used to translate from English to French)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 Sanity check\n",
    "Check if we implemented our model correctly.\n",
    "- Run the model on a small dataset (~2,000 pairs) and\n",
    "- run for a lot of epochs to see if it converges (learns all the responses by heart)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.6 Problems\n",
    "- The bot is very dramatic (thanks to Hollywood screenwriters)\n",
    "- Topics of conversations aren’t realistic\n",
    "- Responses are always fixed for one encoder input\n",
    "- Inconsistent personality\n",
    "- Use only the last previous utterance as the input for the encoder\n",
    "- Doesn’t keep track of information about users"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.7 More refinements\n",
    "#### Train on multiple datasets\n",
    "- [Twitter chat log (courtesy of Marsan Ma)](https://github.com/Marsan-Ma/chat_corpus)\n",
    "- [More movie substitles (less clean)](https://github.com/Marsan-Ma/chat_corpus/)\n",
    "- [Every publicly available Reddit comments (1TB of data!)](https://www.reddit.com/r/datasets/comments/3bxlg7/i_have_every_publicly_available_reddit_comment/)\n",
    "- Your own conversations (chat logs, text messages, emails)\n",
    "\n",
    "#### Chatbot with personalities\n",
    "- At the decoder phase, inject consistent information about the bot\n",
    "- Use the decoder inputs from one person only\n",
    "\n",
    "#### Train on the incoming inputs\n",
    "- Save the conversation with users and train on those conversations\n",
    "- Create a feedback loop so users can correct the bot’s responses\n",
    "\n",
    "#### Remember what users say\n",
    "\n",
    "#### Use characters instead of tokens\n",
    "- Character level language modeling seems to be working quite well\n",
    "- Smaller vocabulary -- no unknown tokens!\n",
    "- But the sequences will be much longer (approximately 4x longer)\n",
    "\n",
    "#### Improve input pipeline\n",
    "- Right now, 50% of running time is spent on generating batches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
