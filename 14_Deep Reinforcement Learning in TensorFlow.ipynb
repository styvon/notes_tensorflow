{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Guest Lecture: Deep Reinforcement Learning in TensorFlow\n",
    "*Danijar Hafner*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Overview\n",
    "- Intro to RL\n",
    "- Value Based Methods\n",
    "- Policy Based Methods\n",
    "- Further Resources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 1. Reinforcement Learning\n",
    "![rl](figures/14_01.png)\n",
    "Difference from supervised learning: no perfect example output\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Formalization as Markov Decision Process\n",
    "#### Environment\n",
    "- Markovian states $s \\in S$\n",
    "- actions $a \\in A$\n",
    "- scalar reward function $R(r_t | s_t, a_t)$\n",
    "- transition function $P(s_{t+1}| s_t, a_t)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Agent\n",
    "- act according to stochastic policy $\\pi(a_t | s_0,...,s_t)$\n",
    "- collects experience tuples $(s_t,a_t,r_t,s_{t+1})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Objective\n",
    "- Maximize expectation of return $R_t=\\sum_{i=0,...,\\infty} \\gamma^i r_{t+1}$ discounted by $0< \\gamma <1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Overview of Methods\n",
    "![methods](figures/14_02.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Value Based Methods\n",
    "### 2.1 Value Learning\n",
    "- value function:  \n",
    "$$V(s_t)=E[R_t]=E[\\sum_{i=0,...,\\infty} \\gamma^i r_{t+i}]$$\n",
    "- Bellman equation:  \n",
    "$$V(s)=r + \\gamma \\sum_{s' \\in S}\\left\\{ P(s' | s, \\pi(a|s))V(s') \\right\\}$$\n",
    "- act according to best $V(s')$, sometimes randomly\n",
    "- estimate $V(s)$ using learning rate:   \n",
    "$$V'(s)= (1- \\alpha)V(s) + \\alpha (r+ V(s'))$$  \n",
    "(Converges to true value function and optimal behavior)\n",
    "- problem: need $P(s'|s)$ to act (as in board games, for example Go)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Q Learning (Watkins89)\n",
    "- Q function: $$Q(s_t, a_t)=E(R_t)$$\n",
    "- Bellman equation: $$Q^* (s,a)=r + \\gamma\\ max_{a' \\in A}Q^*(s', a')$$\n",
    "- act according to best Q(s, a), sometimes randomly\n",
    "- estimate $Q^*(s, a)$ using learning rate:  $$Q'(s, a) = (1 - \\alpha) Q(s, a) + \\alpha (r + max_{a' \\in A} Q(s', a'))$$\n",
    "- converges to optimal function $Q^*(s, a)$ and optimal behavior\n",
    "- doesn't depend on policy, can learn from demonstrations or old experience"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### compare Value Learning & Q Learning\n",
    "![vlql](figures/14_03.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Epsilon Greedy Exploration\n",
    "- convergence and optimality only when visiting each state infinitely often\n",
    "- main challenge in RL: exploration\n",
    "- simple approach: acting randomly with probability $\\epsilon$\n",
    "- visit each $(s, a)$ infinitely often in the limit\n",
    "- decay $\\epsilon$ exponentially to ensure converge\n",
    "- right amount of exploration is often critical in practice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "epsilon = exponential_decay(step, 50000, 1.0, 0.05, rate=0.5)\n",
    "\n",
    "best_action = tf.arg_max(_qvalues([observ])[0], 0)\n",
    "random_action = tf.random_uniform((), 0, num_actions, tf.int64)\n",
    "\n",
    "should_explore = tf.random_uniform((), 0, 1) < epsilon\n",
    "return tf.cond(should_explore, lambda: random_action, lambda: best_action)\n",
    "\n",
    "def exponential_decay(step, total, initial, final, rate=1e-4, stairs=None):\n",
    "    if stairs is not None:\n",
    "        step = stairs * tf.floor(step / stairs)\n",
    "    scale, offset = 1. / (1. - rate), 1. - (1. / (1. - rate))\n",
    "    progress = tf.cast(step, tf.float32) / tf.cast(total, tf.float32)\n",
    "    value = (initial - final) * scale * rate ** progress + offset + final\n",
    "    lower, upper = tf.minimum(initial, final), tf.maximum(initial, final)\n",
    "    return tf.maximum(lower, tf.minimum(value, upper))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Deep Neural Networks\n",
    "#### Nonlinear Function Approximation\n",
    "- rationale: too many states for a lookup table, want to approximate $Q(s, a)$ using a deep neural network\n",
    "- can capture complex dependencies between $s, a$ and $Q(s, a)$ -> agent can learn sophisticated behavior\n",
    "- Convolutional networks for reinforcement learning from pixels\n",
    "    - Share some tricks from papers of the last two years\n",
    "    - Sketch out implementations in TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predicting All Q-Values at Once (Mnih13)\n",
    "Only one forward pass to find the best action\n",
    "![predallq](figures/14_04.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _qvalues(observ):\n",
    "    with tf.variable_scope('qvalues', reuse=True):\n",
    "        # Network from DQN (Mnih 2015)\n",
    "        h1 = tf.layers.conv2d(observ, 32, 8, 4, tf.nn.relu)\n",
    "        h2 = tf.layers.conv2d(h1, 64, 4, 2, tf.nn.relu)\n",
    "        h3 = tf.layers.conv2d(h2, 64, 3, 1, tf.nn.relu)\n",
    "        h4 = tf.layers.dense(h3, 512, tf.nn.relu)\n",
    "        return tf.layers.dense(h4, num_actions, None)\n",
    "    \n",
    "current = tf.gather(_qvalues(observ), action)[:, 0]\n",
    "target = reward + gamma * tf.reduce_max(_qvalues(nextob), 1)\n",
    "target = tf.where(done, tf.zeros_like(target), target)\n",
    "loss = (current - target) ** 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Trick 1: Experience Replay (Mnih13)\n",
    "- stochastic gradient descent expects <u>independent</u> samples\n",
    "- agent collects <u>highly correlated</u> experience at a time\n",
    "- solution: store experience tuples in a large buffer and select random batch for training  \n",
    "-> decorrelates training examples\n",
    "- even better: select training examples prioritized by last training cost (Schaul15)  \n",
    "-> focuses on rare training examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self, template, capacity):\n",
    "        self._capacity = capacity\n",
    "        self._buffers = self._create_buffers(template)\n",
    "        self._index = tf.Variable(0, dtype=tf.int32, trainable=False)\n",
    "        \n",
    "    def size(self):\n",
    "        return tf.minimum(self._index, self._capacity)\n",
    "    \n",
    "    def append(self, tensors):\n",
    "        position = tf.mod(self._index, self._capacity)\n",
    "        with tf.control_dependencies([b[position].assign(t) for b, t in zip(self._buffers, tensors)]):\n",
    "            return self._index.assign_add(1)\n",
    "        \n",
    "    def sample(self, amount):\n",
    "        positions = tf.random_uniform((amount,), 0, self.size - 1, tf.int32)\n",
    "        return [tf.gather(b, positions) for b in self._buffers]\n",
    "    \n",
    "    def _create_buffers(self, template):\n",
    "        buffers = []\n",
    "        for tensor in template:\n",
    "            shape = tf.TensorShape([self._capacity]).concatenate(tensor.get_shape())\n",
    "            initial = tf.zeros(shape, tensor.dtype)\n",
    "            buffers.append(tf.Variable(initial, trainable=False))\n",
    "        return buffers\n",
    "\n",
    "    \n",
    "class PrioritizedReplayBuffer:\n",
    "    def __init__(self, template, capacity):\n",
    "        template = (tf.constant(0.0),) + tuple(template)\n",
    "        self._buffer = ReplayBuffer(template, capacity)\n",
    "    \n",
    "    def size(self):\n",
    "        return self._buffer.size\n",
    "    \n",
    "    def append(self, priority, tensors):\n",
    "        return self._buffer.append((priority,) + tuple(tensors))\n",
    "    \n",
    "    def sample(self, amount, temperature=1):\n",
    "        priorities = self._buffer._buffers[0].value()[:self._buffer.size()]\n",
    "        logprobs = tf.log(priorities / tf.reduce_sum(priorities)) / temperature\n",
    "        positions = tf.multinomial(logprobs[None, ...], amount)[0]\n",
    "        return [tf.gather(b, positions) for b in self._buffer._buffers[1:]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Trick 2: Target Network (Mnih15, Lillicrap16, ...)\n",
    "- targets $r + \\gamma\\ max_{a' \\in A}Q(s', a')$ depend on own current network $Q(s, a)$\n",
    "- training towards moving target makes training unstable\n",
    "- use a moving average $Q^T(s, a)$ of the network to compute the targets\n",
    "- update network parameters $\\theta^T_{t+1}= (1 - \\beta) \\theta^T_t + \\beta \\theta_t$ with $\\beta << 1$\n",
    "- get weights using graph editor and apply `tf.train.ExponentialMovingAverage`\n",
    "- use graph editor to copy network graph and bind to averaged variables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def bind(output, inputs):\n",
    "    for key in inputs:\n",
    "        if isinstance(inputs[key], tf.Variable):\n",
    "            inputs[key] = inputs[key].value()\n",
    "    return tf.contrib.graph_editor.graph_replace(output, inputs)\n",
    "\n",
    "def moving_average(output, decay=0.999, collection=tf.GraphKeys.TRAINABLE_VARIABLES):\n",
    "    average = tf.train.ExponentialMovingAverage(decay=decay)\n",
    "    variables = set(v.value() for v in output.graph.get_collection(collection))\n",
    "    deps = tf.contrib.graph_editor.get_backward_walk_ops(output)\n",
    "    deps = [t for o in deps for t in o.values()]\n",
    "    deps = set([t for t in deps if t in variables])\n",
    "    update_op = average.apply(deps)\n",
    "    new_output = bind(output, {t: average.average(t) for t in deps})\n",
    "    return new_output, update_op\n",
    "\n",
    "current = tf.gather(_qvalues(observ), action)[:, 0]\n",
    "target_qvalues = moving_average(_qvalues(nextob), 0.999)\n",
    "target = reward + gamma * tf.reduce_max(target_qvalues, 1)\n",
    "target = tf.where(done, tf.zeros_like(target), target)\n",
    "loss = (current - target) ** 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Trick 3: Double Q Learning (Hasselt10, Hasselt15)\n",
    "- Q Learning tends to overestimate Q values\n",
    "- same network chooses best action and evaluates it\n",
    "- $r + \\gamma\\ max_{a' \\in A}Q(s', a') = r + \\gamma\\ Q(s', argmax_{a' \\in A}Q(s', a'))$\n",
    "- learning two Q functions from different experience would be ideal\n",
    "- for efficiency, use target network $Q^T(s, a)$ to evaluate action\n",
    "- targets become $r + \\gamma\\ Q^T(s', argmax_{a' \\in A}Q(s', a'))$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Q Learning\n",
    "current = tf.gather(_qvalues(observ), action)[:, 0]\n",
    "target_qvalues = moving_average(_qvalues(nextob), 0.999)\n",
    "target = reward + gamma * tf.reduce_max(target_qvalues, 1)\n",
    "target = tf.where(done, tf.zeros_like(target), target)\n",
    "loss = (current - target) ** 2\n",
    "\n",
    "\n",
    "# Double Q Learning\n",
    "current = tf.gather(_qvalues(observ), action)[:, 0]\n",
    "target_qvalues = moving_average(_qvalues(nextob), 0.999)\n",
    "future_action = tf.argmax(_qvalues(nextob), 1)\n",
    "target = reward + gamma * tf.gather(target_qvalues, future_action)\n",
    "target = tf.where(done, tf.zeros_like(target), target)\n",
    "loss = (current - target) ** 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Policy Based Methods\n",
    "### 3.1 Policy Gradient (Williams92)\n",
    "- learn policy $\\pi(a_t | s_0 , …, s_t)$ directly (instead of value function)\n",
    "- train network to maximize expected return $E[ R_t ]$\n",
    "- $R(r | s, a)$ is unknown but gradient of expectation still possible: $E[ R_t ∇_{\\theta}ln\\ \\pi(a|s) ]$\n",
    "- can only train on-policy because returns won't match otherwise\n",
    "![policybased](figures/14_05.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _policy(observ):\n",
    "    with tf.variable_scope('policy', reuse=True):\n",
    "        # Network from A3C (Mnih 2016)\n",
    "        h1 = tf.layers.conv2d(observ, 16, 8, 4, tf.nn.relu)\n",
    "        h2 = tf.layers.conv2d(h1, 32, 4, 2, tf.nn.relu)\n",
    "        h3 = tf.layers.dense(h2, 256, tf.nn.relu)\n",
    "        cell = tf.contrib.rnn.GRUCell(256)\n",
    "        h4, _ = tf.nn.dynamic_rnn(cell, h3[None, ...], dtype=tf.float32)\n",
    "        return tf.layers.dense(h4[0], num_actions, None)\n",
    "    \n",
    "action_mask = tf.one_hot(action, num_actions)\n",
    "prob_under_policy = tf.reduce_sum(_policy(observ) * action_mask, 1)\n",
    "loss = -return_ * tf.log(prob_under_policy + 1e-13)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Variance Reduction Via Baseline (Williams92, Sutton98)\n",
    "- idea: learn the best actions and don't care about other parts of reward\n",
    "- subtract baseline $b(s)$ from return $R_t$ to reduce variance\n",
    "- advantage actor critic maximizes advantage function $A(s, a) = R_t - V(s)$\n",
    "- in practice, actor and critic often share lower layers\n",
    "\n",
    "![vrvb](figures/14_06.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _shared_network(observ):\n",
    "    with tf.variable_scope('shared_network', reuse=True):\n",
    "        # Network from A3C (Mnih 2016)\n",
    "        h1 = tf.layers.conv2d(observ, 16, 8, 4, tf.nn.relu)\n",
    "        h2 = tf.layers.conv2d(h1, 32, 4, 2, tf.nn.relu)\n",
    "        h3 = tf.layers.dense(h2, 256, tf.nn.relu)\n",
    "        cell = tf.contrib.rnn.GRUCell(256)\n",
    "        h4, _ = tf.nn.dynamic_rnn(cell, h3[None, ...], dtype=tf.float32)\n",
    "        return h4[0]\n",
    "    \n",
    "features = _shared_network(observ)\n",
    "policy = tf.layers.dense(features, num_actions, None)\n",
    "value = tf.layers.dense(features, 1, None)\n",
    "advantage = tf.stop_gradient(return_ - value)\n",
    "action_mask = tf.one_hot(action, num_actions)\n",
    "prob_under_policy = tf.reduce_sum(_policy(observ) * action_mask, 1)\n",
    "policy_loss = -advantage * tf.log(prob_under_policy + 1e-13)\n",
    "value_loss = (return_ - value) ** 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Continuous Control using Policy Gradients\n",
    "- many control problems are better formulated using continuous actions  \n",
    "e.g. control steering angle rather than just left/center/right  \n",
    "- policy gradients don't max over actions as Q Learning does  \n",
    "-> well suited for continuous action spaces  \n",
    "- decompose policy into mean and noise $\\pi(a | s) = \\mu (s) + z(s)$\n",
    "- learn mean and add fixed noise source, or learn both"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Deterministic Policy Gradient (Silver14, Lillicrap16)\n",
    "- continuous policy gradient algorithm that can learn off-policy\n",
    "- evaluate actions using a critic network $Q(s, a)$ rather than the environment  \n",
    "On-policy SARSA doesn't need max over actions!\n",
    "- Backpropagate gradient to the action: $E[ ∇_aQ(s, a) ∇_{\\theta}ln\\ \\pi(s) ]$\n",
    "\n",
    "![dpg](figures/14_07.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features = _shared_network(observ)\n",
    "action = _policy(features, action_size)\n",
    "qvalue = _qvalue(features, action)\n",
    "\n",
    "direction = tf.gradients([qvalue], [action])[0]\n",
    "if self._clip_q_grad:\n",
    "    direction = tf.clip_by_value(direction, -1, 1)\n",
    "target = tf.stop_gradient(action + direction)\n",
    "policy_loss = tf.reduce_sum((target - action) ** 2, 2)\n",
    "\n",
    "target_qvalue = _qvalue(_shared_network(nextob))\n",
    "target_qvalue = moving_average(target_qvalue, 0.999)\n",
    "target = reward + gamma * target_qvalue\n",
    "target = tf.where(done, tf.zeros_like(target), target)\n",
    "loss = (qvalue - target) ** 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Further Resources\n",
    "- Reading\n",
    "    - [Richard Sutton](http://incompleteideas.net/sutton/book/the-book-2nd.html)\n",
    "    - [Andrej Karpathy](http://karpathy.github.io/2016/05/31/rl/)\n",
    "- Lectures\n",
    "    - [David Silver](https://www.youtube.com/watch?v=2pWv7GOvuf0&feature=youtu.be)\n",
    "    - [John Schulman](https://www.youtube.com/watch?v=oPGVsoBonLM&feature=youtu.be)\n",
    "- Software\n",
    "    - [Gym](https://gym.openai.com/)\n",
    "    - [RL Lab](https://github.com/openai/rllab)\n",
    "    - [Modular RL](https://github.com/joschu/modular_rl)\n",
    "    - [Mindpark](https://github.com/danijar/mindpark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
